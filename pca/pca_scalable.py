#!/usr/bin/env python
""" 
A component of a findNeighbour4 server which provides relatedness information for bacterial genomes.
It does so using PCA, and supports PCA based cluster generation.

This is derived from the pca module previously used to
- allow faster processing of samples to generate a matrix for PCA
- constrain memory usage 

The associated classes compute a variation model for samples in a findNeighbour4 server.
Computation uses either
- data in a local copy of sequence data generated by the localstore class, which runs hourly during
routine findNeighbour4 usage, but can also be generated on demand by running findNeighbour4_lsmanager using the --run_once_only option, or
- data from the findNeighbour server database

Functionality is provided in following classes:
* VariationModel - stores results of producing variant matrix and running PCA
* VariantMatrix - computes sample x variant matrix (requires: PERSIST or LocalStore object; server configuration file) 
* PCARunner - runs PCA on VariantMatrix

Unit testing is facilitated by a 
* PersistenceTest class.  This exposes a small subset of the fn3persist object's methods, sufficient to test PCA.  It can be used to store subsets of data for testing purposes 
without then need to access a real fn3persistence data store.

A component of the findNeighbour4 system for bacterial relatedness monitoring
Copyright (C) 2021 David Wyllie david.wyllie@phe.gov.uk
repo: https://github.com/davidhwyllie/findNeighbour4

This program is free software: you can redistribute it and/or modify
it under the terms of the MIT License as published
by the Free Software Foundation.  See <https://opensource.org/licenses/MIT>, and the LICENSE file.
bu
This program is distributed in the hope that it will be useful,
but WITHOUT ANY WARRANTY; without tcen the implied warranty of
MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
GNU Affero General Public License for more details.
"""

# import libraries
import logging
import datetime
from typing import Tuple, Set
import hashlib
import pandas as pd
import numpy as np
import progressbar

from scipy.stats import median_abs_deviation
from sklearn.decomposition import IncrementalPCA
from sklearn.cluster import KMeans

from localstore.pcasequencestore import PCASequenceStore


class VariationModel:
    """Stores a VariantMatrix, the output of a PCA of the matrix, and (optionally) a clustering of the principal components.
    You should not normally have to call this class directly to create a VariationModel - the VariantMatrix class would do this for you.

    - You might wish to instantiate this class directly if you are restoring a previously serialised VariationModel - see constructor"""

    def __init__(self):
        """
        creates a new Variation model.

        """
        self.model = {"built": False}

        return

    def __getitem__(self, key):
        if key not in self.model:
            raise KeyError(f"Key {key} not found")
        return self.model[key]

    def __setitem__(self, key, value):
        """adds a key-value pair to the model"""
        if key in self.model.keys():
            raise KeyError(f"Cannot replace key {key}; existing key is {value}")
        else:
            self.model[key] = value

    def _coefficients_hash(self):
        """computes a hash on the coefficients in the variant model.
        This is useful for version tracking & storing patterns of masking."""
        h = hashlib.md5()
        h.update(self.model["eigenvectors"].to_csv().encode("utf-8"))
        md5_l = h.hexdigest()
        return "{0}".format(md5_l)

    def finish(self):
        """completes construction of the VariationModel"""
        self.model["build_time"] = datetime.datetime.now().isoformat()
        self.model["coefficients_hash"] = self._coefficients_hash()
        self.model["built"] = True


class VariantMatrix:
    """In charge of producing a sample x SNP matrix"""

    def __init__(self, CONFIG, PERSIST, persistdir, show_bar=True):
        """Construct a variant matrix

        Parameters:
        CONFIG: a configuration dictionary, as produced by findn.common_utils.ConfigManager.read_config()
        PERSIST: a persistence object providing ** access ** to stored sequence data.
                 Any of the following will work:

                 findn.mongoStore.fn3persistence object, or
                 findn.rdbmstore.fn3persistence, or
                 localstore.localstoreutils.LocalStore object (fast access from a local tar file - preferred for large datasets)
        persistdir:  a directory in which 'matrix optimised' sequence representations are stored.

        show_bar: whether or not to show a progress bar

        """

        logging.info(
            "Instantiating variant matrix object.  Persistence directory is {0}".format(
                persistdir
            )
        )

        # store the persistence object as part of the object
        self.PERSIST = PERSIST

        # set easy to read properties from the config
        self.analysed_reference_length = len(CONFIG["reference"]) - len(
            set(CONFIG["excludePositions"])
        )
        self.reference_length = CONFIG["reference"]

        # store whether we're using bars for display
        self.show_bar = show_bar

        # fast persistence store
        self.PCASEQSTORE = PCASequenceStore(CONFIG["reference"], persistdir)

        self.vm = VariationModel()
        self.model = {"built": False, "sample_id": []}
        self.validation_data = None

    def set_variationmodel_attribute(self, key, value):
        """sets an attribute of the VariationModel associated with self.vm"""
        self.vm[key] = value

    def get_variationmodel_attribute(self, key):
        """gets an attribute of the VariationModel associated with self.vm"""
        return self.vm[key]

    def guids_for_analysis(self):
        """returns list of guids currently in the persistence object from which to obtain samples"""
        return self.PERSIST.refcompressedsequence_guids()

    def guids_ready_to_analyse(self):
        """returns a list of guids in the PCASEQSTORE ready to analyse"""
        return self.PCASEQSTORE.sequence_ids

    def prepare_to_analyse(self):
        """preprocess valid guids (as returned by guids_for_analysis()) into a format
        which can be analysed rapidly.

        Parameters:
        None

        Returns:
        None"""

        logging.info(
            "Checking which samples need to be converted to a matrix-based format."
        )
        guids_for_analysis = self.guids_for_analysis()
        logging.info(
            "There are {0} samples available to analyse in set-based reference compressed sequence format".format(
                len(guids_for_analysis)
            )
        )

        guids_ready_to_analyse = self.guids_ready_to_analyse()
        logging.info(
            "There are {0} samples available to analyse in sparse array compressed sequence format".format(
                len(guids_ready_to_analyse)
            )
        )
        still_to_analyse = guids_for_analysis - guids_ready_to_analyse
        logging.info(
            "Creating ready-to-analyse (sparse array) data for {0} samples".format(
                len(still_to_analyse)
            )
        )
        if self.show_bar:
            bar = progressbar.ProgressBar(max_value=len(still_to_analyse))
        num_loaded = 0
        for sequence_id, rcs in self.PERSIST.read_many(still_to_analyse):
            if sequence_id is None:
                break  # last record
            num_loaded += 1
            if rcs["invalid"] == 0:
                self.PCASEQSTORE.store_rcs(sequence_id, rcs)
                if self.show_bar:
                    bar.update(num_loaded)

        self.PCASEQSTORE.flush()  # flush any in-memory buffered items to file

        if self.show_bar:
            bar.finish()

        logging.info("Preparation for analysis is complete")

    def _column_name(self, psn_dict):
        """given a base at a position, returns a position:base string suitable for use as a pandas column name"""
        pos = psn_dict["pos"]
        nt = psn_dict["nt"]
        return f"{pos}:{nt}"

    def get_position_counts(
        self, guids=None, starting_result=None
    ) -> Tuple[set, dict, dict]:
        """returns positions of variation across the genome

        Parameters:
        guids : a set of sequence identifiers to analyse. If None, all samples in self.PERSIST are analysed
        starting_result: the result of a previous call to get_position_counts.  If supplied, previously analysed sequences will not be re-analysed.

        Method of operation:
        If the guid
        Returns:
        a dictionary containing
            sequence_ids analysed
            vmodel (positions of variation)
            allelemodel (positions of variation, and the associated base)
        """
        if guids is None:
            logging.info("Summarising variation model for all samples")
            return self.PCASEQSTORE.summarise_all()
        else:
            logging.info("Summarising variation model for selected samples")
            return self.PCASEQSTORE.summarise_many(
                select_sequence_ids=guids, starting_result=starting_result
            )

    def get_missingness_cutoff(
        self, positions: Set[int], mmodel: np.array, fold_over_median
    ) -> int:
        """computes a missingness cutoff, applicable at a per-sequence ltcel.

        Samples which have high levels of missingness (i.e. N, -, or IUPAC mixture codes)
        may be unsuitable for incorporation into PCA models.  Some level of missingness is expected,
        but samples with very high missingness may compromise modelling.

        Parameters:
        positions: a set of integers, representing the positions across which missingness is to be estimated
        mmodel:    a missingness model, which is an array [0, 223, 120 ... , 0] where counts are the number of missing data items at each point
                   across the data set analysed.

        Returns:
                   a estimate of how many missing positions are unexpected.  This is set to fold_over_median * median missingness.
                   Note:      This criterion is somewhat arbitrary.
                   The impact of this approximation has not been systematically tcaluated, and could be the subject of further work.
        """

        missingness = [x for i, x in enumerate(mmodel[0]) if i in positions]
        median_missingness = np.median(
            missingness
        )  # study median, as this will be relatively insensitive to samples with high missingness

        upper_cutoff = fold_over_median * median_missingness
        return upper_cutoff

    def prepare(
        self,
        min_variant_freq=None,
        num_train_on=None,
        select_from=None,
        exclude_positions_with_missingness_fold_over_median=10,
        target_matrix_size=10000,
        min_matrix_size=1000,
    ):
        """
        Prepares to build a matrix, returning a series of chunks, for PCA
        To recover the matrix, call matrix_in_blocks()

        input:
            min_variant_freq: the minimum proportion of samples with variation at that site for the site to be included.  If none, is set to 3/train_on, i.e. each variant has to appear 3 times to be considered
            num_train_on: only compute PCA on a subset of train_on samples.  Set to None for all samples.
            deterministic:  if num_train on is not None, setting deterministic = True (default) ensures the same samples are analysed each time.  If num_train_on is None, has no effect.
            select_from: only build a model from the sample_ids in the list provided. If None, has no effect
            exclude_positions_with_missingness_fold_over_median: exclude any positions of variation if they have more than this number of times the median missingness
            target_matrix_size: the target number of rows in each block of rows which are returned by the generator
            min_matrix_size: the minimum number of rows in each matrix returned by the generator
        output:
            guids, a list of sample_ids (guids) in the matrix

        Note:
            call self.matrix_in_blocks(guids) to recover the matrix in chunks"""

        # determine guids there in the database
        guids = self.PCASEQSTORE.sequence_ids
        self.target_matrix_size = target_matrix_size
        self.min_matrix_size = min_matrix_size
        self.matrix_sequence_id_order = []

        if select_from is not None:
            if not (isinstance(select_from, list) or isinstance(select_from, set)):
                raise TypeError(
                    "Select from must be list or set, not {0}".format(type(select_from))
                )
            select_from = set(select_from)

            if len(select_from) > 0:  # if it is zero, we assume everything is required.
                guids = set(guids).intersection(select_from)
                guids = list(guids)
            else:
                logging.warning(
                    "Asked to sub-select from an empty list.  Using all samples"
                )
        else:
            logging.info(
                "No sub-selection specified; analysing all samples in the sequence store"
            )
        # check whether there are any samples
        if len(guids) == 0:
            raise ValueError(
                "No samples found in the PCA sequence store.  You may need to populate this by calling .prepare_to_analyse() first"
            )

        ########################################################################################################
        # if we have been told to use a subset of these to build the model, construct that subset.
        logging.info(
            "Identified {0} samples after any sub-selection".format(len(guids))
        )
        if num_train_on is None:  # if we are not told how many to use then we use
            num_train_on = len(guids)  # all samples
            logging.info("Working with all {0} samples".format(num_train_on))
        else:

            if num_train_on == 0:  # analyse all
                num_train_on = len(guids)  # all samples
                logging.info("Working with all {0} samples".format(num_train_on))
            elif num_train_on < len(guids):
                logging.info(
                    "Identifying a subset of {0} samples to build pca model on".format(
                        num_train_on
                    )
                )
                guids = guids[:num_train_on]
        self.set_variationmodel_attribute(
            "num_train_on", num_train_on
        )  # persist parameters used
        #########################################################################################################

        #########################################################################################################
        # if minimum variation is not set, only analyse variants seen at least 2 times.
        if min_variant_freq is None:
            min_variant_freq = 2 / num_train_on
        #########################################################################################################

        #########################################################################################################
        # determine the variation model.  In the first stage, we analyse by position
        # positions with unexpectedly high ltcels of missingness are excluded, as these may be hard to call.
        logging.info("Assessing per-base variation from {0} samples".format(len(guids)))
        summary_statistics = self.get_position_counts(guids)

        logging.info(
            "Identified variation in {0} sites. There were {1} allelic variants. {2} sites had missing (N/IUPAC/-) data.".format(
                np.sum(np.sign(summary_statistics["vmodel"])),
                np.sum(np.sign(summary_statistics["allelemodel"])),
                np.sum(np.sign(summary_statistics["mmodel"])),
            )  # the number of positions with Ns
        )

        # store variant model
        self.set_variationmodel_attribute(
            "variant_frequencies", summary_statistics["vmodel"][0]
        )
        self.set_variationmodel_attribute("min_variant_freq", min_variant_freq)
        self.set_variationmodel_attribute(
            "analysed_reference_length", self.PCASEQSTORE.reference_sequence_length
        )

        # from the totality of the variation, select positions with > cutoff % variation
        cutoff_variant_number = num_train_on * min_variant_freq
        self.set_variationmodel_attribute(
            "cutoff_variant_number", cutoff_variant_number
        )

        select_alleles = [
            i
            for i, x in enumerate(summary_statistics["allelemodel"][0])
            if x >= cutoff_variant_number
        ]
        logging.info(
            "Found {0} alleles which occur in >= {1} sequences ({2}).".format(
                len(select_alleles), cutoff_variant_number, min_variant_freq
            )
        )

        if len(select_alleles) == 0:
            raise ValueError(
                "No variation found above cutoff. normally this is because you ran the PCA operation against an empty database; this is what happens if you omit a config file parameter, when a test database is examined by default.   Cannot continue"
            )

        self.set_variationmodel_attribute(
            "variant_alleles_gt_cutoff_variant_number", len(select_alleles)
        )

        # map alleles to positions
        select_positions = sorted(
            set([self.PCASEQSTORE.identify_variant(x)["pos"] for x in select_alleles])
        )

        upper_cutoff = self.get_missingness_cutoff(
            positions=select_positions,
            mmodel=summary_statistics["mmodel"],
            fold_over_median=exclude_positions_with_missingness_fold_over_median,
        )  # define cutoff
        self.set_variationmodel_attribute("max_ok_missingness", float(upper_cutoff))
        self.set_variationmodel_attribute(
            "max_ok_missingness_pc", int(100 * upper_cutoff / num_train_on)
        )

        # remove any positions with high levels of missingness from the variation model to be used in the PCA
        num_removed = 0
        exclude_positions = [
            x
            for x in select_positions
            if summary_statistics["mmodel"][0, x] >= upper_cutoff
        ]
        logging.info(
            "Identified {0} nucleotide positions and {1} to exclude with a max missingness cutoff of {2} sequences".format(
                len(select_positions), len(exclude_positions), upper_cutoff
            )
        )
        remaining_positions = sorted(set(select_positions) - set(exclude_positions))

        exclude_alleles = []
        for exclude_position in exclude_positions:
            for nt in ["A", "C", "G", "T"]:
                # logging.info("{0} {1}".format(nt, exclude_position))
                exclude_alleles.append(
                    self.PCASEQSTORE._variant_int_id(nt, exclude_position)
                )

        include_alleles = set(select_alleles) - set(exclude_alleles)
        num_removed = len(select_alleles) - len(include_alleles)
        self.select_alleles = include_alleles
        pc_removed = int(100 * (num_removed / (len(select_positions))))
        logging.info(
            f"Removed {num_removed} positions with missingness > cutoff of {upper_cutoff} sequences.  Represents removal of {pc_removed} %"
        )
        logging.info(
            f"There remain {len(self.select_alleles)} alleles which vary at frequencies more than {min_variant_freq} and pass the missingness cutoff."
        )

        # store model parameters
        self.set_variationmodel_attribute(
            "allele_positions_ok_missingness", sorted(list(include_alleles))
        )
        self.set_variationmodel_attribute("positions_in_model", remaining_positions)
        self.set_variationmodel_attribute(
            "selected_variation",
            [
                self._column_name(self.PCASEQSTORE.identify_variant(x))
                for x in include_alleles
            ],
        )
        self.set_variationmodel_attribute(
            "selected_variation_dict",
            [self.PCASEQSTORE.identify_variant(x) for x in include_alleles],
        )

        #########################################################################################################
        # call self.matrix_in_blocks(guids) to return result
        return guids

    def matrix_in_blocks(self, guids):
        """returns the variation matrix, in blocks

        Parameters:
        guids: the sample identifiers with which to build the matrix

        Yields:
        tuples, consisting of matrix_row_names, which are guids, and a variation matrix

        Side Effect:
        sets the sample_info attribute of the variation model.  This is a pandas dataframe, comprising

        sample_id: string
        n_in_model: int
        model_positions: int

        used_in_pca: True
        suspect_quality: False

        """

        # build a variation matrix for variant sites which pass, and samples which pass
        logging.info(
            "Constructing matrices of {1} rows from {0} samples".format(
                len(guids), self.target_matrix_size
            )
        )

        # determine the positions of variation, in the allele map
        modelled_variation_sites = self.get_variationmodel_attribute(
            "positions_in_model"
        )
        modelled_allelic_sites = self.get_variationmodel_attribute(
            "allele_positions_ok_missingness"
        )

        n_loaded = 0
        n_this_block = 0
        matrix_rows = []
        matrix_row_names = []
        if self.show_bar:
            bar = progressbar.ProgressBar(max_value=len(guids))
        self.matrix_sequence_id_order = []
        self.matrix_sequence_properties = {}

        near_end = (
            len(guids) - self.min_matrix_size
        )  # don't break if more than this number have been added, as the block will be small

        for sequence_id, result in self.PCASEQSTORE.read_many(
            guids
        ):  # read sparse matrices describing sequence
            n_loaded += 1
            n_this_block += 1

            allelicvariation = result["variants"].toarray()[
                0
            ]  # extract the positions of variants
            modelled_allelicvariation = allelicvariation[
                modelled_allelic_sites
            ]  # and subpositions representing the variation

            nvariation = result["ns"].toarray()[0]  #  total Ns
            modelled_nvariation = nvariation[modelled_variation_sites]

            self.matrix_sequence_properties[sequence_id] = dict(
                sample_id=sequence_id,
                model_positions=len(modelled_variation_sites),
                non_reference_positions=sum(modelled_allelicvariation),
                n_in_model=sum(modelled_nvariation),
                used_in_pca=True,
                suspect_quality=False,
            )
            matrix_rows.append(modelled_allelicvariation)
            matrix_row_names.append(sequence_id)
            self.matrix_sequence_id_order.append(sequence_id)
            if self.show_bar:
                bar.update(n_loaded)

            # if we have accrued enough samples, or we are within min_matrix_size of the last sample
            if n_this_block >= self.target_matrix_size and n_loaded < near_end:
                yield matrix_row_names, np.array(matrix_rows)
                matrix_rows = []
                matrix_row_names = []
                n_this_block = 0

        if n_this_block > 0:
            yield matrix_row_names, np.array(matrix_rows)  # the last chunk

        self.matrix_sequence_properties = pd.DataFrame.from_dict(
            self.matrix_sequence_properties, orient="index"
        )
        logging.info("Matrix construction complete.")


class PCARunner:
    """Performs PCA using a VariantMatrix object"""

    def __init__(self, snp_matrix: VariantMatrix, show_bar=True):
        self.vm = snp_matrix
        self.transformed_coordinates = None
        self.show_bar = show_bar

    def run(
        self,
        n_components,
        select_from=None,
        pca_parameters={},
        target_matrix_size=10000,
        exclude_positions_with_missingness_fold_over_median=12,
    ) -> VariationModel:
        """conducts pca on a snp_matrix, storing the results in the snp_matrix's VariantModel object.

        input:
            n_components: the maximum number of components to extract.
            select_from: an iterable of sample identifiers.  If None, all samples are analysed
            pca_parameters: a dictionary of parameters passed to the scikit-learn PCA command
                            The contents of the dictionary are passed as-is to the PCA command, without any checking.
            target_matrix_size: the target number of rows in each block of rows which are processed each time.  Parameter passed to VariantMatrix.build()
            min_matrix_size: the minimum number of rows in each matrix returned by the generator. Parameter passed to VariantMatrix.build()
            exclude_positions_with_missingness_fold_over_median : criterion for excluding positions.  Parameter passed to VariantMatrix.build()

        returns:
            VariationModel
        """

        logging.info("Performing pca, extracting {0} components".format(n_components))
        self.vm.set_variationmodel_attribute("n_pca_components", n_components)

        min_matrix_size = (
            3 * n_components
        )  # target.  If matrix_size < n_components, won't fit

        # performs incremental PCA allowing essentially unlimited sample size
        # see https://stackoverflow.com/questions/31428581/incremental-pca-on-big-data
        t0 = datetime.datetime.now()
        pca = IncrementalPCA(n_components=n_components, **pca_parameters)
        guids = self.vm.prepare(
            select_from=select_from,
            target_matrix_size=target_matrix_size,
            min_matrix_size=min_matrix_size,
            exclude_positions_with_missingness_fold_over_median=exclude_positions_with_missingness_fold_over_median,
        )
        blocks_fitted = 0
        for variantmatrix_row, variantmatrix in self.vm.matrix_in_blocks(guids):
            blocks_fitted += 1
            logging.info(
                "Fitting block {0} of {1} rows".format(
                    blocks_fitted, len(variantmatrix)
                )
            )
            pca.partial_fit(variantmatrix)
        self.vm.set_variationmodel_attribute(
            "sample_id", self.vm.matrix_sequence_id_order
        )  # note guids responsible
        self.vm.set_variationmodel_attribute(
            "sample_info", self.vm.matrix_sequence_properties
        )
        t1 = datetime.datetime.now()
        elapsed = (t1 - t0).total_seconds()
        logging.info("PCA and matrix construction took {0} seconds".format(elapsed))
        contribs = []

        pc2contributing_pos = {}
        contributing_basepos = set()
        contributing_pos = set()
        vardict = self.vm.get_variationmodel_attribute("selected_variation_dict")

        for i, row in enumerate(pca.components_, 0):
            # mark values far from the median, which is close to zero
            # this information is not used, but it is retained for depiction purposes if neededls
            row_median = np.median(row)
            row_mad = median_abs_deviation(row)
            row_upper_ci = row_median + 5 * row_mad
            row_lower_ci = row_median - 5 * row_mad

            pc2contributing_pos[i] = set()
            for j, cell in enumerate(row, 0):

                if cell > row_upper_ci or cell < row_lower_ci:
                    outside_5mad = True
                else:
                    outside_5mad = False
                pos = vardict[j]["pos"]
                allele = vardict[j]["nt"]
                colhead = "{0}:{1}".format(pos, allele)

                # indicate whether positions are strongly weighted
                # the outside_5mad criterion is arbitrary
                if outside_5mad is True:
                    pc2contributing_pos[i].add(pos)
                    contributing_basepos.add(colhead)
                    contributing_pos.add(pos)
                    contribs.append(
                        {
                            "pc": i,
                            "pos": pos,
                            "allele": allele,
                            "col": colhead,
                            "weight": cell,
                            "outside_5mad": outside_5mad,
                        }
                    )
            pc2contributing_pos[i] = sorted(
                list(pc2contributing_pos[i])
            )  # can be json serialised, unlike set

        # report eigenvectors which are different from median +- 3 median absolute dtciations
        self.eigenvectors = pd.DataFrame.from_records(contribs)

        if len(self.eigenvectors.index) == 0:
            raise KeyError(
                "PCA problem.  No eigenvectors found.  Contributions found are as follows: {0}.  This usually means there is insufficient data to build PCs.  Try increasing the sample number".format(
                    contribs
                )
            )

        # compute transformed_coordinates for the samples on which the fit was performed.
        logging.info("Computing transformed_coordinates")
        transformed_coordinates_dict = {}
        sample_ids = self.vm.get_variationmodel_attribute("sample_id")

        # variant_matrix does not exist - need to recover it
        for sample_ids, variant_matrix in self.vm.matrix_in_blocks(
            sample_ids
        ):  # read  matrices describing sequence
            for guid, tcs in zip(sample_ids, pca.transform(variant_matrix)):
                transformed_coordinates_dict[guid] = tcs

        self.transformed_coordinates = pd.DataFrame.from_dict(
            transformed_coordinates_dict, orient="index"
        )
        self.transformed_coordinates.columns = range(n_components)

        self.vm.set_variationmodel_attribute("pca", pca)
        self.vm.set_variationmodel_attribute(
            "transformed_coordinates", self.transformed_coordinates
        )
        self.vm.set_variationmodel_attribute("eigenvectors", self.eigenvectors)
        self.vm.set_variationmodel_attribute(
            "explained_variance_ratio", list(pca.explained_variance_ratio_)
        )
        self.vm.set_variationmodel_attribute(
            "n_contributing_positions", len(contributing_pos)
        )
        self.vm.set_variationmodel_attribute(
            "pc2_contributing_positions", pc2contributing_pos
        )
        self.vm.set_variationmodel_attribute(
            "n_contributing_variants", len(contributing_basepos)
        )
        self.vm.set_variationmodel_attribute(
            "contributing_basepos", contributing_basepos
        )
        self.vm.set_variationmodel_attribute("contributing_pos", contributing_pos)
        self.vm.set_variationmodel_attribute(
            "pos_per_pc",
            [
                len(x)
                for x in self.vm.get_variationmodel_attribute(
                    "pc2_contributing_positions"
                ).values()
            ],
        )

        logging.info(
            "PCA completed, identified {0} strongly contributing base/positions".format(
                len(contributing_basepos)
            )
        )
        self.vm.vm.finish()

        return self.vm.vm

    def cluster(self, initial_cats_per_unit=8):
        """clusters the transformed_coordinates obtained by run()

        Categorises transformed_coordinates.  Uses kmeans clustering, and uses a crude approximation to estimate the number of clusters.

        The technique used operates per pc; we bin transformed_coordinates into bins 1/initial_cats_per_unit wide, and count the non-zero bins.  This is used as an estimate of
        the numbers of clusters, and the pca is provided with the bin centres as a set of starting values.

        Empirically, the technique was found to provide better discrimination of emerging SARS-CoV-2 genomes than approaches based on model fitting,
        such as Gaussian mixture modelling, although it undoubtedly splits some PCs arbitrarily.

        Parameters:
            initial_cats_per_unit: (default 8).  Used to estimate the number of k-means clusters to generate.

        Outputs:
            self.vm: the VariantModel object generated by this routine

        Sets:
            self.transformed_coordinate_categories: a data frame containing cluster names for each cluster

        """

        # check there is a model
        if self.transformed_coordinates is None:
            raise NotImplementedError(
                "No transformed_coordinates.  You must call .run() before calling .cluster()"
            )

        t0 = datetime.datetime.now()  # startup time

        # prepare data for clustering
        tc = (
            self.transformed_coordinates.copy()
        )  # transformed_coordinates.  option to drop PCs of technical origin could be dropped.
        if self.show_bar:
            bar = progressbar.ProgressBar(
                max_value=len(self.transformed_coordinates.columns.to_list())
            )

        logging.info("Clustering transformed_coordinates")
        for i, col in enumerate(tc.columns):

            if self.show_bar:
                bar.update(i)

            this_tc = tc[col].to_frame()
            this_tc.columns = ["transformed_coordinate"]
            this_tc["pc"] = col
            this_tc["initial_cat"] = [
                int(x * initial_cats_per_unit)
                for x in this_tc["transformed_coordinate"]
            ]

            # how many non-zero categories
            cats = this_tc.groupby(["initial_cat"])["transformed_coordinate"].describe()

            # convert to arrays to fit
            to_fit = this_tc["transformed_coordinate"].to_numpy().reshape(-1, 1)
            centres = cats["mean"].to_numpy().reshape(-1, 1)
            km = KMeans(n_clusters=len(cats.index), n_init=1, init=centres).fit(to_fit)
            this_tc["cat"] = km.labels_
            this_tc["sample_id"] = this_tc.index

            # store a pc_cat field.  useful for searching later.
            pc_cats = [str(col) + "_" + str(x) for x in this_tc["cat"]]
            this_tc["pc_cat"] = pc_cats
            this_tc.drop(["initial_cat"], axis=1)
            if col == 0:
                tcs = this_tc

            else:
                tcs = tcs.append(this_tc, ignore_index=True)

        self.vm.set_variationmodel_attribute("transformed_coordinate_categories", tcs)
        self.vm.vm.finish()
        if self.show_bar:
            bar.finish()

        t1 = datetime.datetime.now()
        elapsed = (t1 - t0).total_seconds()
        logging.info(
            "Transformed coordinate clustering took {0} seconds".format(elapsed)
        )

        return self.vm.vm
