#!/usr/bin/env python
""" 
A component of a findNeighbour3 server providing relatedness information for bacterial genomes.

The associated classes compute a variation model for samples in the findNeighbour3 server.
Computation uses data in MongoDb, and is not memory intensive.

It loads configuration from a config file, which must be set in production.

If no config file is provided, it will run in  'testing' mode with the  parameters
in default_test_config.json.  This expects a mongodb database to be running on
the default port on local host.  As a rough guide to the amount of space required in mongodb,
about 0.5MB of database is used per sequence, or about 2,000 sequences per GB.

Four classes are proposed:
* ModelBuilder - produces VariantModels (requires: PERSIST object for mongodb access; parameters) 
* VariationModel - the weights generated by a PCA (requires: nothing) [TODO: serialise/deseraialise]
* pcaComparer - a class comparing the results of PCA computations (requires: a VariationModel)
* pcaValidater - a class comparing the results of PCA vs. snv computations (requires: PERSIST object; a VariationModel).


"""
 
# import libraries
import os
import sys
import requests
import json
import logging
import warnings
import datetime
import glob
import time
import sys
import hashlib
import queue
import threading
import random
import gc
import io
import pymongo
import pandas as pd
import numpy as np
import copy
import pathlib
import markdown
import codecs
import sentry_sdk
import matplotlib
import dateutil.parser
import argparse
import networkx as nx
import progressbar
import bokeh
from scipy.stats import binom_test
from sklearn import linear_model
from sklearn.decomposition import SparsePCA, PCA, TruncatedSVD
from sklearn.cluster import KMeans
from sentry_sdk import capture_message, capture_exception
from sentry_sdk.integrations.flask import FlaskIntegration

# logging
from logging.config import dictConfig

# utilities for file handling and measuring file size
import psutil

# reference based compression, storage and clustering modules
from NucleicAcid import NucleicAcid
from mongoStore import fn3persistence
from seqComparer import seqComparer		# import from seqComparer_mt for multithreading
from clustering import snv_clustering
from guidLookup import guidSearcher  # fast lookup of first part of guids

# network visualisation
from visualiseNetwork import snvNetwork

# server status visualisation
from depictStatus import MakeHumanReadable

# only used for unit testing
from Bio import SeqIO
from Bio.Seq import Seq
from Bio.SeqRecord import SeqRecord
from Bio.Alphabet import generic_nucleotide
import unittest
from urllib.parse import urlparse as urlparser
from urllib.parse import urljoin as urljoiner
import uuid
import time

class MNStats():
	""" computes the number of M and N bases in a reference compressed object """
	def __init__(self, select_positions, analysed_reference_length):
		""" input:
			select_positions are the positions contributing to the pca model, as generated by ModelBuilder.
			analysed_reference_length are the number of reference bases analysed. """
		self.select_positions = select_positions
		self.analysed_reference_length = analysed_reference_length
	def examine(self, obj):	
		""" examines the reference compressed object obj,
		reporting
		* number of Ns and Ms in the sequence, subdivided by whether they are in 
		  selected_positions
		* Test 2 (binomial test, as per findNeighbour3) testing whether the frequency of Ns/Ms in the selected_positions exceed those elsewhere. """

		missing= {'M_in_model':0,
		'N_in_model':0, 					'model_positions':len(self.select_positions), 						'reference_positions':self.analysed_reference_length}

		for base in ['M','N']:		# compute missingness

			# record total numbers of N and M per guid
			try:
				missing["{0}_total".format(base)] = len(obj[base])
			except KeyError:
				missing["{0}_total".format(base)] = 0

			# examine all missing (N/M) sites, adding to a missingness model
			try:
				for pos in obj[base]:
					if pos in self.select_positions:
						try:
							missing["{0}_in_model".format(base)] +=1							
						except KeyError:
							pass

			except KeyError:
				pass		# if there are no M,N then we can ignore these

			# compute an factor which could be used to adjust an eigenvector 
			# based on the number of Ns assuming Ns are missing at random.
			try:
				missing["{0}_eigenvalue_inflation_parameter".format(base)]  = len(self.select_positions)/(len(self.select_positions)-missing["{0}_in_model".format(base)])
			except KeyError:		# no N/M
				missing["{0}_eigenvalue_inflation_parameter".format(base)]  = 1

			## do binomial test
			not_model = self.analysed_reference_length- len(self.select_positions)
			p_expected = (missing["{0}_total".format(base)]-missing["{0}_in_model".format(base)])/not_model
			missing["{0}_expected_proportion".format(base)]=p_expected
			in_model = len(self.select_positions)
			p_observed = missing["{0}_in_model".format(base)]/len(self.select_positions)
			missing["{0}_observed_proportion".format(base)]=p_observed
			p_val = binom_test(
				missing["{0}_in_model".format(base)], 
				len(self.select_positions), 
				p_expected, 
				alternative='greater')
 
			missing["{0}_p_value".format(base)]=p_val
			
		return missing
class VariationModel():
	""" represents the output of a PCA, as performed by model builder """ 
		
	def __init__(self,serialised_representation = None):
		""" creates a variant model, either:
			* which is empty, if serialised representation is None, or which is complete,
			* using a serialised version of a previous VariantModel.

			If creating a model from scratch, create an empty model, and then use the .add() method to add components of it,
			calling .finish() on the model when you are complete.
		"""
		if serialised_representation is None:
			self.model = {'built':False}
		else:
			self.model = self.deserialise(serialised_representation)	
		return
	def add(self, key, value):
		""" adds a key-value pair to the model """
		if key in self.model.keys():
			raise KeyError("Cannot replace key {0}".format(key))
		else:
			self.model[key] = value
		return
	def _coefficients_hash(self):
		""" computes a hash on the coefficients in the variant model.
		This is useful for version tracking & storing patterns of masking. """
		h = hashlib.md5()
		h.update(self.model['eigenvectors'].to_csv().encode('utf-8'))
		md5_l = h.hexdigest()
		return("{0}".format(md5_l))

	def finish(self):
		""" completes construction of the VariationModel """
		self.model['build_time'] = datetime.datetime.now().isoformat()
		self.model['coefficients_hash'] = self._coefficients_hash()
		self.model['built'] = True

	def serialise(self):
		""" serialise to dictionary compatible with json representation """
		raise NotImplementedError
	def deserialise(self, received_dict):
		""" transform to native format based on dictionary produced by serialise """
		raise NotImplementedError

class ModelBuilder():
	""" builds a variant model from reference mapped data, and performs PCA on it """ 
		
	def __init__(self,CONFIG, PERSIST):
		""" Using values in CONFIG, estimates based on distances in the server with CONFIG['NAME'] on port CONFIG['PORT'].
		
		Note that the findNeighbour3 server itself does not have to be operational for this class to work, but the underlying database does have to be present.

		related to error handling:
		SENTRY_URL (optional)
		Note: if a FN_SENTRY URL environment variable is present, then the value of this will take precedence over any values in the config file.
		This allows 'secret' connstrings involving passwords etc to be specified without the values going into a configuraton file.
		PERSIST is a storage object needs to be supplied.  The fn3Persistence class in mongoStore is one suitable object.
		PERSIST=fn3persistence(connString=CONFIG['FNPERSISTENCE_CONNSTRING'])

		"""
		
		# store the persistence object as part of the object
		self.PERSIST=PERSIST
		
		# check input
		if isinstance(CONFIG, str):
			self.CONFIG=json.loads(CONFIG)	# assume JSON string; convert.
		elif isinstance(CONFIG, dict):
			self.CONFIG=CONFIG
		else:
			raise TypeError("CONFIG must be a json string or dictionary, but it is a {0}".format(type(CONFIG)))
		
		# check it is a dictionary	
		if not isinstance(self.CONFIG, dict):
			raise KeyError("CONFIG must be either a dictionary or a JSON string encoding a dictionary.  It is: {0}".format(CONFIG))
		
		# check that the keys of config are as expected.
		required_keys=set(['IP','INPUTREF','EXCLUDEFILE','DEBUGMODE','SERVERNAME',
						   'FNPERSISTENCE_CONNSTRING', 'MAXN_STORAGE',
						   'SNPCOMPRESSIONCEILING', "SNPCEILING", 'MAXN_PROP_DEFAULT', 'REST_PORT',
						   'LOGFILE','LOGLEVEL','GC_ON_RECOMPRESS','RECOMPRESS_FREQUENCY', 'REPACK_FREQUENCY', 'CLUSTERING'])
		missing=required_keys-set(self.CONFIG.keys())
		if not missing == set([]):
			raise KeyError("Required keys were not found in CONFIG. Missing are {0}".format(missing))

		# load reference
		cfg = self.PERSIST.config_read('config')
		
		# set easy to read properties from the config
		self.analysed_reference_length = len( cfg['reference']) - len(set(cfg['excludePositions']))

		# we start without any variation model
		self._reset()
		
	def _reset(self):
		""" clears existing variation model and pca result """ 
		self.vm = VariationModel()		
		self.pcs = None
		self.eigenvectors = None
		self._invalid = set()	# invalid guids for which we can't compute pcs
		self.model = {'built':False, 'built_with_guids':[]}

		self.validation_data = None

	def guids(self):
		""" returns list of guids currently in the findNeighbour3 database"""
		return sorted(self.PERSIST.refcompressedsequence_guids())	# sorting is not essential, but makes more deterministic for testing
		
	def _column_name(self,pos,base):
		""" given a base at a position, returns a position:base string suitable for use as a pandas column name """ 			
		return	"{0}:{1}".format(pos,base)
			
	def build(self, n_components='mle', min_variant_freq=None, train_on = None):
		""" builds a variant model, conducts pca, and stores the output in a VariantModel object.

			input: 
				n_components: the maximum number of components to extract.  if set to 'mle' (default) the method of Minke is used see https://vismod.media.mit.edu/tech-reports/TR-514.pdf
				min_variant_freq: the minimum proportion of samples with variation at that site for the site to be included.  If none, is set to 10/train_on, i.e. each variant has to appear 10 times to be considered
				train_on: only compute PCA on a subset of train_on samples.  Set to None for all samples.
			returns:
				a VariantModel object
			
		"""
		
		# determine guids there in the database
		guids = self.guids()
		
		# randomise order for model training purposes
		random.shuffle(guids)
		
		if train_on is None:
			train_on = len(guids)
		
		# persist parameters used
		self.vm.add('train_on',train_on)
		self.vm.add('n_components', n_components)
		
		# set minimum variation as 1/ (0.1* number of samples)
		if min_variant_freq is None:
			min_variant_freq = 10/train_on
		
		# build an array containing the amount of variation and missingness (M/N) across bases.
		vmodel = {}		# variants
		mmodel = {}		# missingness
		guids_analysed_stage1 = set()
		nLoaded = 0
		print(">>Determining variant sites, from a derivation set of up to {0} samples ".format(train_on))
		bar = progressbar.ProgressBar(max_value = train_on)
		for guid in guids:
			nLoaded+=1

			if nLoaded>=train_on:		# trained on enough samples
				break
				pass
			bar.update(nLoaded)
			obj = self.PERSIST.refcompressedsequence_read(guid)	# ref compressed sequence

			if not obj['invalid'] == 1:
				guids_analysed_stage1.add(guid)
				# for definite calls, compute variation at each position
				for base in ['A','C','G','T']:
					try:
						for pos in obj[base]:
							try:
								vmodel[pos]=vmodel[pos]+1
							except KeyError:
								if not pos in vmodel.keys():
									vmodel[pos]={}
								vmodel[pos]=1

					except KeyError:
						pass		# if there are no A,C,G or T then we can ignore these
				for base in ['M','N']:		# compute missingness

						# examine all missing (N/M) sites, adding to a missingness model
					try:
						for pos in obj[base]:
							try:
								mmodel[pos]=mmodel[pos]+1
							except KeyError:
								if not pos in vmodel.keys():
									mmodel[pos]={}
								mmodel[pos]=1

					except KeyError:
						pass		# if there are no M,N then we can ignore these
		bar.finish()

		# store variant model
		self.vm.add('variant_frequencies', vmodel)
		self.vm.add('min_variant_freq', min_variant_freq)
		self.vm.add('analysed_reference_length', self.analysed_reference_length)
		# from the totality of the the variation, select positions with > cutoff % variation
		cutoff_variant_number = nLoaded*min_variant_freq
		self.vm.add('cutoff_variant_number', cutoff_variant_number)

		select_positions = set()
		for pos in vmodel.keys():
			if vmodel[pos]>=cutoff_variant_number:

				select_positions.add(pos)
		print("Found {0} positions which vary at frequencies more than {1}.".format(len(select_positions),min_variant_freq))

		if len(select_positions) == 0:
			raise ValueError("No variation found above cutoff. normally this is because you ran the PCA operation against an empty database; this is what happens if you omit a config file parameter, when a test database is examined by default.   Cannot continue")

		self.model['variant_positions_gt_cutoff_variant_number'] = len(select_positions)
		# analyse missingness in these
		missingness = []
		for pos in select_positions:
			try:
				missingness.append(mmodel[pos])
			except KeyError:		# no missing data at this position
				missingness.append(0)
		mean_missingness = np.mean(missingness)
		sd_missingness = np.sqrt(mean_missingness)		# poisson assumption
		upper_cutoff = int(2*sd_missingness + mean_missingness)	# normal approximation
		print("Identified max acceptable missingness as {0} sequences ({1}%)".format(upper_cutoff,int(100*upper_cutoff/train_on)))
		self.vm.add('max_ok_missingness', upper_cutoff)
		self.vm.add('max_ok_missingness_pc', int(100*upper_cutoff/train_on))
	
		# remove any positions with missingness more than this
		to_remove=[]
		for pos in select_positions:
			try:
				if mmodel[pos]>upper_cutoff:
					to_remove.append(pos)
			except KeyError:
				pass 	# no missing data at this point
		for item in to_remove:
			select_positions.remove(item)
		self.select_positions = select_positions

		self.mns = MNStats(select_positions, self.vm.model['analysed_reference_length'] )
		print("Found {0} positions which vary at frequencies more than {1} and pass missingness cutoff.".format(len(select_positions),min_variant_freq))
		self.vm.add('variant_positions_ok_missingness', len(select_positions))

		# find any samples which have a high number of missing bases
		print(">> scanning for sequences with per-sample unexpectedly high missingness (N), or likely to be mixed (M)")
		bar = progressbar.ProgressBar(max_value = len(guids_analysed_stage1))
		
		guid2missing = {}
		for nLoaded,guid in enumerate(guids_analysed_stage1):
			bar.update(nLoaded)
			obj = self.PERSIST.refcompressedsequence_read(guid)	# ref compressed sequence
			for base in ['M','N']:		# compute missingness

				# examine all missing (N/M) sites, adding to a missingness model
				try:
					for pos in obj[base]:
						if pos in select_positions:
							try:
								mmodel[pos]=mmodel[pos]+1
							except KeyError:
								if not pos in vmodel.keys():
									mmodel[pos]={}
								mmodel[pos]=1

				except KeyError:
					pass		# if there are no M,N then we can ignore these

			## do binomial test and n count
			guid2missing[guid] = self.mns.examine(obj)

		bar.finish()

		# collate mixture quality information, and identify low quality (mixed, \
		#	as judged by high Ns or Ms in the variant sites)
		mix_quality_info = pd.DataFrame.from_dict(guid2missing, orient='index')
		self.vm.add('mix_quality_info',mix_quality_info)

		# identify any mixed samples.  we don't build the model from these.
		# mixed are defined as having significantly more N or M in the variant
		# bases than in other bases.
		mix_quality_cutoff = 0.05 / len(mix_quality_info.index)		# Bonferroni adj
			# maybe better to use FDR
		suspect_quality = mix_quality_info.query('M_p_value < {0} or N_p_value < {0}'.format(mix_quality_cutoff))
		guids_analysed_stage2= guids_analysed_stage1 - set(suspect_quality.index.to_list())

		# build a variation matrix for variant sites
		vmodel = {}
		nLoaded = 0
		print(">>Gathering variation for matrix construction from {0} unmixed samples ".format(len(guids_analysed_stage2)))

		bar = progressbar.ProgressBar(max_value = train_on)
		
		self.model['built_with_guids'] = []
		for guid in guids_analysed_stage2:
			nLoaded+=1
			if nLoaded>=train_on:		# if we're told only to use a proportion, and we've analysed enough,
				break
				pass
			bar.update(nLoaded)

			obj = self.PERSIST.refcompressedsequence_read(guid)	# ref compressed sequence
			# for definite calls, compute variation at each position
			
			# for invalid samples, compute nothing
			if obj['invalid'] == 1:
				self._invalid.add(guid)
			else:
				# compute a variation model - a list of bases and variants where variation occurs
				variants={}		# variation for this guid

				# for definite calls, compute variation at each position
				# positions of variation where a call was made
				for base in set(['A','C','G','T']).intersection(obj.keys()):
					target_positions = select_positions.intersection(obj[base])
					called_positions = dict((self._column_name(pos,base),1) for pos in target_positions) 
					variants = {**variants, **called_positions}
					
				vmodel[guid]=variants
		bar.finish()

		print(">>Building variant matrix")	
		vmodel = pd.DataFrame.from_dict(vmodel, orient='index')
		vmodel.fillna(value=0, inplace=True)	# if not completed, then it's reference
							# unless it's null, which we are ignoring at present
		vmodel = vmodel.drop_duplicates()	# deduplicate by row
		self.vm.add('null_elements_in_vmodel', vmodel.isna().sum().sum())
		self.vm.add('vmodel', vmodel)
		
		if len(select_positions)>len(vmodel.index) and n_components == 'mle':
			# then we can't use Minke's method, although we were told to
			n_components = max(int(len(vmodel.index)/20),2)		# default rule of thumb:  component for every 20 samples, or a min of 2
			print("Defaulting to {0} components, as Minke's method cannot be applied and no n_components was specified.".format(n_components))
			self.vm.add('n_pca_components_choice', 'Minke method')

		else:
			self.vm.add('n_pca_components_choice', 'rule based, one per every 20 samples')
	
		print(">>Performing pca extracting {0} components".format(n_components))
		self.vm.add('n_pca_components', n_components)
			
		pca = PCA(n_components=n_components, svd_solver='auto') 	# if too large, can use IncrementalPCA
		pca.fit(vmodel)

		contribs = []
		nz_columns = {}
		for i,row in enumerate(pca.components_,0):
			if sum(row)>0:
				for j,cell in enumerate(row,0):
					if cell>0:
						contribs.append({'pc':i, 'pos': int(vmodel.columns[j].split(':')[0]), 'col':vmodel.columns[j], 'weight':cell})
		
		# eigenvectors
		self.eigenvectors = pd.DataFrame.from_records(contribs)
		try:
			self.eigenvectors.set_index('col',drop=True, inplace=True)
		except KeyError:
			# no vectors extracted?
			raise KeyError("PCA problem.  Column not not found.  Contributions found are as follows: {0}.  This usually means there is insufficient data to build PCs.  Try increasing the sample number".format(contribs))
		# ~ maximum length of the eigenvector [not sure this is useful]
		pc_scores= self.eigenvectors.groupby('pc')['weight'].agg(['sum','count'])
		pc_scores.columns = ['pc_maxscore','pc_max_n_contributing']

		# explained variance
		explained_variance = pd.DataFrame({'pc':list(range(len(pca.explained_variance_))), 'explained_var':pca.explained_variance_})
		explained_variance.set_index('pc',drop=True, inplace=True)
		explained_variance = explained_variance.merge(pc_scores, left_index=True, right_index=True)

		# compute eigenvalues for the data on which the fit was performed.
		print(">>Computing eigenvalues for {0} unmixed samples ".format(len(guids_analysed_stage2)))
		eigenvalues_dict = {}
		for guid, evs in zip(vmodel.index, pca.transform(vmodel)):
			eigenvalues_dict[guid] = evs
		eigenvalues = pd.DataFrame.from_dict(eigenvalues_dict, orient= 'index')
		eigenvalues.columns = range(n_components)

		# consider eigenvector abs sum; experimental statistic reflecting model prediction.
		# used as a diagnostic to evaluate how Ns affect the model
		eigenvalue_distance_metric = eigenvalues.abs().sum(axis=1).to_frame()
		eigenvalue_distance_metric.columns=['eigenvalue_distance_metric']
		self.vm.add('eigenvalue_quality_info',
			mix_quality_info.merge(eigenvalue_distance_metric, left_index=True, right_index=True))
		
		# inflate each element in the eigenvalue by the relevant eigenvalue_inflation_parameter.  this is (for tb) typically less than 1%
		print(">>Adjusting eigenvalues for {0} unmixed samples for missingness ".format(len(guids_analysed_stage2)))
	
		tmp = eigenvalues.merge(mix_quality_info, left_index=True, right_index=True, how='inner')
		eigenvalues = eigenvalues.multiply(tmp['N_eigenvalue_inflation_parameter'], axis = 0)

		self.vm.add('pca', pca)
		self.vm.add('explained_variance', explained_variance)
		self.vm.add('eigenvalues', eigenvalues)	
		self.vm.add('eigenvectors', self.eigenvectors)
		self.vm.add('pcs', pc_scores)
		contributing_basepos = list(set(self.eigenvectors.index))
		self.vm.add('n_contributing_positions', len(contributing_basepos))
		self.vm.add('contributing_basepos', contributing_basepos)
		self.vm.add('contributing_pos', set(self.eigenvectors['pos']))
		self.vm.add('built_with_guids', vmodel.index.tolist())

		print("PCA examined {0} contributing base/positions".format(len(contributing_basepos)))
		self.vm.finish()
		return self.vm

class pcaComparer():	
	""" compares samples using a pca based method.

	Where possible interface is the same as seqComparer, which compares using a snv based method.
	
	It uses a VariationModel object, which is created separately by the ModelBuilder class.  

	The VariationModel contains weightings which, when applied to variants between the reference genome and the sequence of interest, allow computation of principle components for each sequence.

	The most important methods available in this class are:
	* compress - computes pca for a sequence
	* persist - stores the pca in ram
	* compare and mcompare - compare pairs, or all, samples, respectively.

	Example usage:
	# in this example, sequences are recovered from a findNeighbour3 database
	try:
			PERSIST=fn3persistence(dbname = CONFIG['SERVERNAME'],
						     connString=CONFIG['FNPERSISTENCE_CONNSTRING'],
						        debug=CONFIG['DEBUGMODE'],
							server_monitoring_min_interval_msec = 0)
	except Exception as e:
			print("Error raised on creating persistence object")
			raise

	# make builder

	builder = ModelBuilder(CONFIG, PERSIST)
	model = builder.build(train_on=500)

	pcc = pcaComparer(model=model)

	all_guids = set(PERSIST.refcompressedsequence_guids())
	training_guids = set(model.model['built_with_guids'])
	guids = all_guids - training_guids
	bar = progressbar.ProgressBar(max_value = len(guids))
	
	analysed =[]
	for i,guid in enumerate(guids):
		obj = PERSIST.refcompressedsequence_read(guid)	# ref compressed sequence
		pcs = pcc.compress(guid, obj)
		pcc.persist(pcs, guid)
		bar.update(i)
		analysed.append(guid)
		if i> 5:
			break
	bar.finish()


	for i in sorted(analysed):
		for j in sorted(analysed):
			print(i,j,pcc.countDifferences_byKey((i,j)))

	for retval in pcc.mcompare(analysed[0]):
		print(retval)

	for retval in pcc.distmat():
		print(retval)
	exit()

	 """

	def __init__(self, model, what = 'ACTG'):
		""" creates a pcaComparer object.  
		input:
			model is a VariationModel object generated by BuildModel
			what: what kind of calls to compress.
					options: 'ACTG'  - confident calls only [ default ] 
						 'M'     - components of bases only
						 'all'	 - ACTG and M bases
		"""
		self.vm = model
		self.compressed_sequence_keys = set(['invalid','A','C','G','T', 'N', 'M'])
		self.expected_compress_output_keys = set(['mix_quality_info','eigenvalues'])
		self.what = what
		self._refresh()		# where the sequences are stored
		self.mns = MNStats(self.vm.model['contributing_pos'], self.vm.model['analysed_reference_length'] )
		
		# iupac codes from https://www.bioinformatics.org/sms/iupac.html
		self.iupac = {
			'R':['G','A'],
			'W':['A','T'],
			'Y':['T','C'],
			'M':['A','C'],
			'S':['A','C'],
			'K':['G','T']
			}	
		
	def _refresh(self):
		""" removes any new data from pcaComparer other than that existing in the model."""

		# if what is 'ACTG' (which is what the model is built with, then we can use model coefficients.
		#if self.what == 'ACTG':
		#	self.pcaArray = self.vm.model['eigenvalues'].copy()
		#	self.qualityScoring = self.vm.model['mix_quality_info'].copy()
		#else:
		self.pcaArray = pd.DataFrame(columns = self.vm.model['eigenvalues'].columns.to_list())
		self.qualityScoring = pd.DataFrame(columns = self.vm.model['mix_quality_info'].columns.to_list())

	def persist(self, obj):
		""" stores one or more pca compressed objects into RAM.  The pca compressed object should be generated by compress()
		"""

		if obj is None:
			return False


		if not set(obj.keys()) == self.expected_compress_output_keys :
			raise KeyError("A dictionary was passed to .persist, but it does not have the required keys {0}.  Its keys are :".format(self.expected_compress_output_keys), set(obj.keys()))

		# will raise error if duplicates are entered.
		try:
			# obj is the summary is generated by .compress()
			self.pcaArray = self.pcaArray.append(obj['eigenvalues'], verify_integrity = True)

		except ValueError:
			return
		try:
			self.qualityScoring = self.qualityScoring.append(obj['mix_quality_info'], verify_integrity=True)

		except ValueError:
			return

	def remove(self, guid):
		""" removes a pca compressed object from ram """
		try:
		       pass
		except KeyError:
		       pass 	# we permit attempts to delete things which don't exist
		## TODO: remove from self.pcaArray

	def load(self, guid):
		""" returns a pca compressed object  """
		return pd.DataFrame.from_dict(self.pcaArray[guid])

	def iscachedinram(self,guid):
		""" returns true or false depending whether we have a local copy of the refCompressed representation of a sequence (name=guid) in this machine """
		if guid in self.pcaArray.index():
			return(True)
		else:
			return(False)
	def guidscachedinram(self):
		""" returns all guids with sequence profiles currently in this machine """
		retVal=set()
		for item in self.pcaArray.index():
			retVal.add(item)
		return(retVal)
	def _guid(self):
		""" returns a new guid, generated de novo """
		return(str(uuid.uuid1()))

	def _column_name(self,pos,base):
		""" given a base at a position, returns a position:base string suitable for use as a pandas column name """ 			
		return	"{0}:{1}".format(pos,base)

	def _compress_one(self, guid, obj):
		""" computes principal components of sequence, identified by guid, with the  variation model stored in self.vm.
			input: 
				guid: identifies the sequence to be compared
				obj: a reference compressed object, as generated from a sequence by the SeqComparer class, or as read from persistence, e.g.
		# now we compute the pcas
		obj = self.PERSIST.refcompressedsequence_read(guid)	# ref compressed sequence
				
			returns:
				the principal components for this guid, or None if the sequence is invalid
		
			raises:
				ValueError if a variation model, as computed by compute_variation_model, does not exist.
				KeyError is obj is not of the correct format
				
		"""
		
		# test whether obj is of the right format
		if len(set(obj.keys()) - self.compressed_sequence_keys) > 0:
			raise TypeError("obj should have keys in {0} but has keys {1}".format(obj.keys(), self.compressed_sequence_keys))

		# test whether there is a variant model stored.
		if self.vm.model['built'] is False:
			raise ValueError("No variation model computed: must construct this with compute_variation_model before calling .compare()")
		
		if obj['invalid'] is False:
			return None		# invalid, can't compute, no need to reload from disc
				
		if guid in self.pcaArray.index:
			return None		# no need to compress


		# no variation at all positions
		target_columns = self.vm.model['vmodel'].columns.to_list()
		variation_row = dict(zip(target_columns, [0]*len(target_columns)))

		# get relevant calls from this sequence for the positions used by the PCA.
		vmodel = []
		vbasepos = []
		if self.what in ['ACTG','all']:
			for base in set(obj.keys()).intersection(set(['A','C','G','T'])):		# for each base which is present; Ms and Ns are not scored.
				variant_positions = self.vm.model['contributing_pos'].intersection(obj[base])	# for each position which is variant and is also in the model
				for pos in variant_positions: 
					variation_row[self._column_name(pos,base)]=1				# compute the name of the relevant column
					

		if self.what in ['M','all']:
			for base in set(obj.keys()).intersection(set(['M'])):		# for each base which is present; Ms and Ns are not scored.
				variant_positions = self.vm.model['contributing_pos'].intersection(obj[base].keys())	# for each position which is variant and is also in the model
				for pos in variant_positions:
					# the M base contains an iupac code representing the variation 
					try:
						these_bases = self.iupac[obj[base][pos]]
						for this_base in these_bases:
							variation_row[self._column_name(pos,this_base)]=1
					except KeyError: 		# not a diallelic iupac; ignore this
						pass
		return(variation_row)

	def compress(self, guid, obj, enable_bar=True):
		""" computes principal components of sequence, identified by guid, with the  variation model stored in self.vm.
			input: 
				guid: identifies the sequence to be compared, OR a list of such objects
				
				obj: a reference compressed object, OR a list of such objects, as generated from a sequence by the SeqComparer class, or as read from persistence, e.g.
		# now we compute the pcas
		obj = self.PERSIST.refcompressedsequence_read(guid)	# ref compressed sequence
				
			returns:
				the principal components for this guid, or None if the sequence is invalid
		
			raises:
				ValueError if a variation model, as computed by compute_variation_model, does not exist.
				KeyError is obj is not of the correct format
		

		if enable_bar is true (default) will display progress bar if >1 object to compress

		returns
			dictionary consisting of quality statistics and eigenvalues.
			{'mix_quality_info':missing_df,'eigenvalues':eigenvalues}	
		"""
		
		
		# coerce into a format suitable for PC computation
		if isinstance(obj,dict):
			objs = [obj]
			guids = [guid]
		else:
			objs = obj
			guids = guid

		use_bar = len(guids)>1 and enable_bar

		if use_bar:
			print("Performing multiple compression ..")
			bar = progressbar.ProgressBar(max_value = len(guids))

		vm = {}
		missing = {}
		n = 0
		for guid,obj in zip(guids, objs): 
			one_row = self._compress_one(guid, obj)
			if not one_row is None:		# compression is necessary
				vm[guid]= one_row
				missing[guid] = self.mns.examine(obj)
				n+=1
			if use_bar:
				bar.update(n)
		if use_bar:
			bar.finish()

		# compute eigenvalues
		if len(vm.keys())==0:		# we did nothing, as all done already
			return None

		vm_df = pd.DataFrame.from_dict(vm, orient='index')
		target_columns = self.vm.model['vmodel'].columns.to_list()
		vm_df = vm_df[target_columns]
		evs = self.vm.model['pca'].transform(vm_df)
		evs_dict = {}
		for (guid,evs_row) in zip(vm_df.index.to_list(),evs):
			evs_dict[guid]=evs_row

		eigenvalues = pd.DataFrame.from_dict(evs_dict, orient= 'index')
		eigenvalues.columns = list(range(len(eigenvalues.columns)))
		
		# inflate each element in the eigenvalue by the relevant eigenvalue_inflation_parameter.  this is (for tb) typically less than 1%
		# compute the eigenvector inflation vector

		mix_quality_info = pd.DataFrame.from_dict(missing, orient='index')
		tmp = eigenvalues.merge(mix_quality_info, left_index=True, right_index=True, how='inner')
		eigenvalues = eigenvalues.multiply(tmp['N_eigenvalue_inflation_parameter'], axis = 0)
		return {'mix_quality_info':mix_quality_info,'eigenvalues':eigenvalues}

	
	def countDifferences_byKey(self, keyPair):
		""" compares the in memory pcaCompressed sequences at identified by a tuple (key1,key2)
		self.seqProfile[key1] and self.seqProfile[key2]

		Returns the pairwise distance, derived from the pca distance in a tuple (key1,key2,pca_dist)
		"""

		# we use mcompare, as it is the fastest method
		(key1,key2) = keyPair
		try:
			scores = self.mcompare(key1,[key2])
			if len(scores.index)==1:
				return((key1,key2,scores['pca_dist'][0]))
			else:
				scores.to_csv('scores_error.csv')
				self.pcaArray.to_csv('scores_pcaArray.csv')

				raise ValueError("Problem: scores is not one row .. see scores_error.csv")
				
		except KeyError:
			raise
			return None	## if either key's entry is None, then the sequences are invalid; we return None

	def mcompare(self, guid, guids=None, method = 1):
		""" performs comparison of one guid with guids; if guids are not specified, we use all
		stored guids
		"""
		# raise an error if there is no model 	
		if self.vm.model['built'] is False:
			raise ValueError("No variation model computed: must construct this with compute_variation_model before calling .compare()")

		# if guids are not specified, we do all vs all
		if guids is None:
			guids = list(self.pcaArray.index.to_list())
			
		# if guids are specified, check they exist.  Not existing is an error
		# this computation will slow up the calculation and could potentially be disabled, replaced with try/catch for KeyErrors
		missing = set(guids)-set(self.pcaArray.index.to_list())
		if len(missing)>0:
			raise KeyError("mcompare: Asked to compare with non existing samples: {0}.  sample must be compressed and .persist() called before comparison; index holds {1}.".format(missing, sorted(self.pcaArray.index.to_list())))
		if not guid in self.pcaArray.index:
			raise KeyError("mCompare: Asked to compare {0}  but guid requested has not been stored.  call .persist() on the sample to be added before using mcompare.")
		# end of optional data checking section

		target = self.pcaArray.loc[guids] - self.pcaArray.loc[guid]		# this is the difference between the components' scores.  												# Formerly took logs and subtracted the median.  This latter is a method of controlling for assay level variability, but if such variation is substantial
											# it will prevent us using k-means approaches later

		#medians = target.median(axis=1, skipna=True)
		#target = target.sub(medians,axis=0)
		target = target.apply(abs)						# we are concerned with fold change from the median

		x= self.vm.model['explained_variance']['explained_var']			# multiply by SNP associated variation for each pc
		target = target.mul(x, axis=1)		# multiply all values by x, row wise
		
		scores = target.sum(axis=1, skipna=True)
		scores = scores.to_frame()
		scores.columns = ['pca_dist']
		scores['guid2'] = guid
		scores['guid1'] = scores.index.tolist()
		return scores

	def distmat(self, half=True, diagonal=False):
		""" returns a distance matrix.
		parameters
		If half=True, returns only half the matrix.
		If diagonal=True, return the diagonal (all of which are zeros)

		could refactor to support multithreading.

		returns:
		A generator yielding neighbours, in format [guid1,guid2,dist]
		"""

		for guid in self.pcaArray.index():
			scores = self.mcompare(guid, None)
			for guid1 in scores.index:
				guid2 = scores.at[guid1, 'guid2']
				include = False
				if not half and not guid1==guid2:
					include = True
				if guid1==guid2 and diagonal:
					include = True
				if guid1<guid2 and half:
					include = True

				if include:
					## TODO: suppress invalid scores ??
					yield [guid1,guid2, scores.at[guid1,'pca_dist']]
         
class pcaValidator():
	""" validate predictions from pca vs. snv gold standard """
	def __init__(self, CONFIG, PERSIST, model, what):
		""" creates a  pc validator object """
		self.PERSIST = PERSIST
		self.vm = model
		
# check input
		if isinstance(CONFIG, str):
			self.CONFIG=json.loads(CONFIG)	# assume JSON string; convert.
		elif isinstance(CONFIG, dict):
			self.CONFIG=CONFIG
		else:
			raise TypeError("CONFIG must be a json string or dictionary, but it is a {0}".format(type(CONFIG)))
		
		# check it is a dictionary	
		if not isinstance(self.CONFIG, dict):
			raise KeyError("CONFIG must be either a dictionary or a JSON string encoding a dictionary.  It is: {0}".format(CONFIG))
		
		# check that the keys of config are as expected.
		required_keys=set(['IP','INPUTREF','EXCLUDEFILE','DEBUGMODE','SERVERNAME',
						   'FNPERSISTENCE_CONNSTRING', 'MAXN_STORAGE',
						   'SNPCOMPRESSIONCEILING', "SNPCEILING", 'MAXN_PROP_DEFAULT', 'REST_PORT',
						   'LOGFILE','LOGLEVEL','GC_ON_RECOMPRESS','RECOMPRESS_FREQUENCY', 'REPACK_FREQUENCY', 'CLUSTERING'])
		missing=required_keys-set(self.CONFIG.keys())
		if not missing == set([]):
			raise KeyError("Required keys were not found in CONFIG. Missing are {0}".format(missing))

		# the following keys are not stored in any database backend, as a server could be moved, i.e.
		# running on the same data but with different IP etc
		
		do_not_persist_keys=set(['IP',"SERVERNAME",'FNPERSISTENCE_CONNSTRING',
								 'LOGFILE','LOGLEVEL','REST_PORT',
								 'GC_ON_RECOMPRESS','RECOMPRESS_FREQUENCY', 'REPACK_FREQUENCY', 'SENTRY_URL', 'SERVER_MONITORING_MIN_INTERVAL_MSEC'])
				
		# load global settings from those stored at the first run.
		cfg = self.PERSIST.config_read('config')
		
		# set easy to read properties from the config
		self.reference = cfg['reference']
		self.excludePositions = set(cfg['excludePositions'])
		self.debugMode = cfg['DEBUGMODE']
		self.maxNs = cfg['MAXN_STORAGE']
		self.snpCeiling = cfg['SNPCEILING']
		self.snpCompressionCeiling = cfg['SNPCOMPRESSIONCEILING']
		self.maxn_prop_default = cfg['MAXN_PROP_DEFAULT']
		self.clustering_settings = cfg['CLUSTERING']
		self.recompress_frequency = self.CONFIG['RECOMPRESS_FREQUENCY']
		self.repack_frequency = self.CONFIG['REPACK_FREQUENCY']
		self.gc_on_recompress = self.CONFIG['GC_ON_RECOMPRESS']
		
	
		# initialise seqComparer, which manages in-memory reference compressed data
		self.sc=seqComparer(reference=self.reference,
							maxNs=self.maxNs,
							snpCeiling= self.snpCeiling,
							debugMode=self.debugMode,
							excludePositions=self.excludePositions,
							snpCompressionCeiling = self.snpCompressionCeiling)

		self.what = what
		what = list(set(what) - set(['M']))		# this version computes M components by default.  if 'M' is supplied as an option, it doesn't have any effect
		self.whatM = what
		self.whatM.append('M')

		self.pcc = {}
		for item in what:
			print("Creating pcaComparer for ",item)
			self.pcc[item] = pcaComparer(model= model, what = item)


	def validate(self, sample_size=30, internal= False,snp_cutoff=12, pca_cutoff=None):
		"""	produces a comparison of exact SNP distances vs. estimates from the current PC model.  
		
		uses sampling strategy.  by default, does not study guids on which the PCA model is constructed,
		i.e. uses external validation.

		sample_size:  the number of samples to be studied

		Note that:
		guids will be loaded into RAM using a local seqComparer object, which will consume memory. [could be avoided, but validation will run slowly]
		the resulting comparison table is held in RAM until it is written.  This could easily be avoided"""

		# find all sequences in the linked database (mongo, created by findNeighbour3)

		all_guids = set(self.PERSIST.refcompressedsequence_guids())
		training_guids = set(self.vm.model['built_with_guids'])

		if internal is False:		# validate on external data set
			sampling_frame = all_guids - training_guids
			print("Sampling {1} from {0} samples, which exclude samples used to build the model".format(len(sampling_frame), sample_size))
		else:
			sampling_frame = training_guids	
			print("Sampling {1} from  {0} samples used to build the model".format(len(sampling_frame), sample_size))
		bar = progressbar.ProgressBar(max_value = sample_size)
		
		nneighbours=1
		print("Loading selected samples, and up to {0} close neighbours within {1} snp..".format(nneighbours, self.snpCeiling))
		n_sampled = 0
		guids_to_study = {}
		compression_time=0
		nCompressed = 0
		objs= []
		guids_to_study = []
		while n_sampled < sample_size and len(sampling_frame)>0:
			candidate = random.sample(sampling_frame, 1)[0]
			sampling_frame.remove(candidate)

			obj = self.PERSIST.refcompressedsequence_read(candidate)	# ref compressed sequence
			if not obj['invalid']==1:
				n_sampled += 1
				bar.update(n_sampled)
				objs.append(obj)
				guids_to_study.append(candidate)
				self.sc.persist(obj, candidate)

				# determine if it has close neighbours; select up to three of them
				neighbours = self.PERSIST.guid2neighbours(candidate, cutoff= self.snpCeiling, returned_format = 3)['neighbours']
				add_neighbours = random.sample(neighbours, min(nneighbours,len(neighbours)))

				for neighbour in add_neighbours:
					obj = self.PERSIST.refcompressedsequence_read(neighbour)	# ref compressed sequence
					if not obj['invalid']==1:
						objs.append(obj)
						guids_to_study.append(neighbour)
						self.sc.persist(obj, neighbour)

		bar.finish()

		print("Computing eigenvalues")
		for item in self.whatM:
			t1 = time.perf_counter()
			res = self.pcc[item].compress(guids_to_study, objs)
			self.pcc[item].persist(res)
			t2 = time.perf_counter()
			nCompressed += len(guids_to_study)
			compression_time = compression_time + (t2-t1)


		print("PC computation time per sample was {0} seconds over {1} sequences".format(compression_time/nCompressed, nCompressed))	


		# generate distance matrix 
		comparisons = []
		print("Comparing SNP with PCA distances in pairwise comparisons.")
		l = len(guids_to_study)
		bar = progressbar.ProgressBar(max_value = l*l )
		nLoaded =0
		for guid1 in guids_to_study:
			for guid2 in guids_to_study:
				nLoaded +=1
				if guid1 < guid2:	#half matrix only
					(guid1,guid2,snp_dist,n1,n2,nboth,n1pos,n2pos,nbothpos) = self.sc.countDifferences_byKey(keyPair=(guid1,guid2), cutoff = 5e6)
					add_dict = {'guid1':guid1,'guid2':guid2, 'snp_dist':snp_dist}
					for item in self.what:	
		
						(guid1,guid2,pca_dist) = self.pcc[item].countDifferences_byKey(keyPair=(guid1,guid2))
						add_dict['pca_dist_{0}'.format(item)] = pca_dist

						qs1 = self.pcc[item].qualityScoring.loc[guid1].to_dict()
						qs2 = self.pcc[item].qualityScoring.loc[guid1].to_dict()
						for key in qs1:
							add_dict["guid1_"+key]=qs1[key]
							add_dict["guid2_"+key]=qs1[key]	

					comparisons.append( add_dict)  
					bar.update(nLoaded)

		bar.finish()

		self.validation_data = pd.DataFrame.from_records(comparisons)

		self.for_analysis = {}
		pca_cutoff= {}

		# compute maximum distance observed
		for item in self.what:
			pca_dist = 'pca_dist_{0}'.format(item)		# name of the field
			max_pca_dist = max(self.validation_data[pca_dist])
			max_snp_dist = max(self.validation_data['snp_dist'])
			est_ratio = max_pca_dist / max_snp_dist

			# if pca_cutoff is not supplied, then we compute it.
			tmp = self.validation_data.query("{1} < {0}".format(snp_cutoff+1, pca_dist))

			# compute maximum pca_dist within zero snp items
			pca_dists = tmp[pca_dist].tolist()
			pca_dists = self.removeOutliers(pca_dists, outlierConstant= 2.5)

			autoset_method = None
			if max(pca_dists) > 0:
				# if non zero, set upper limit at 5 sigma based on outlier-removed data with SNV <= 12
				pca_cutoff[item] = 1+np.mean(pca_dists)+5*(np.std(pca_dists))		# minimum of 5 sigma higher
				autoset_method = 'Five sigma using outlier-removed pairs with low pairwise SNP distances'
			else:		# no data- everything is zero
				pca_cutoff[item] =  1.5* snp_cutoff * est_ratio			# based on average slope
				autoset_method = ' estimated slope of pca to snp distance line'
			print("During validation, for {2} we have autoset pca_dist cutoff at {0} using {1}".format(pca_cutoff[item], autoset_method, item))

			# find subset with high est distances and low snp distances.  Do a forensic analysis of what happened.
			self.for_analysis[item] = self.validation_data.query("snp_dist < {0} and {2} > {1}".format(snp_cutoff,pca_cutoff[item], pca_dist))
			problematic_guids = set()
			for ix in self.for_analysis[item].index:
				print(self.for_analysis[item].loc[ix,'guid1'],self.for_analysis[item].loc[ix,'guid2'],
					self.for_analysis[item].loc[ix,pca_dist],self.for_analysis[item].loc[ix,'snp_dist'])

				problematic_guids.add(self.for_analysis[item].loc[ix,'guid1'])
				problematic_guids.add(self.for_analysis[item].loc[ix,'guid2'])

		return pca_cutoff

	def removeOutliers(self, x, outlierConstant=1.5):
		""" remove outliers from data """
		a = np.array(x)
		upper_quartile = np.percentile(a, 75)
		lower_quartile = np.percentile(a, 25)
		IQR = (upper_quartile - lower_quartile) * outlierConstant
		quartileSet = (lower_quartile - IQR, upper_quartile + IQR)
		resultList = []
		for y in a.tolist():
			if y >= quartileSet[0] and y <= quartileSet[1]:
				resultList.append(y)
		return resultList
	
	def depict_validation(self, filename, snp_ceiling, pca_ceiling):
		""" exports the validation data, if any, to a file """
		if self.validation_data is None:

			return None 	# needs to be computed first

		else:
			# prepare data for plotting
			cmp = self.validation_data
			snp_ceiling = self.snpCeiling
			xtarget = cmp['snp_dist']<=snp_ceiling
			low_snpdist = cmp[xtarget]
			ymax = max(cmp['pca_dist'])
			xmax = max(cmp['snp_dist'])
			ytarget = cmp['pca_dist']<=pca_ceiling
			cmp['xcat'] = xtarget
			cmp['ycat'] = ytarget
			ns = pd.crosstab(cmp['xcat'],cmp['ycat'])
			upper_y_pos = pca_ceiling + (ymax - pca_ceiling)/2

			# put on quadrant counts
			ns_dict = {1:0, 2:0, 3:0, 4:0}
			try:
				ns_dict[1] = ns.loc[True,True]
			except KeyError:
				pass

			try:
				ns_dict[2] = ns.loc[False,True]
			except KeyError:
				pass
			try:
				ns_dict[3] = ns.loc[True,False]
			except KeyError:
				pass
			try:
				ns_dict[4] = ns.loc[False,False]
			except KeyError:
				pass
			annotation = pd.DataFrame({
					'n':[ns_dict[1],ns_dict[2],ns_dict[3],ns_dict[4]],
					'y':[pca_ceiling/2,pca_ceiling/2, upper_y_pos, upper_y_pos],
					'x':[-4*snp_ceiling,xmax/2, -4*snp_ceiling, xmax/2]})

			source_an = bokeh.models.ColumnDataSource(annotation)
			source    = bokeh.models.ColumnDataSource(cmp)

			# plot
			p = bokeh.plotting.figure(title = "Validation")
			p.xaxis.axis_label = 'SNV distance (computed by findNeighbour3)'
			p.yaxis.axis_label = 'PCA distance (arbitrary units) '
			p.circle(x="snp_dist", y="pca_dist",source=source, fill_alpha=0.2, size=3)
			glyph = bokeh.models.glyphs.Text(x="x", y="y", text="n", angle=0, text_color="black")
			p.add_glyph(source_an, glyph)

			vline = bokeh.models.Span(location=snp_ceiling, dimension= 'height', line_color='green', line_width=1)
			hline = bokeh.models.Span(location=pca_ceiling, dimension= 'width', line_color='green', line_width=1)
					
			p.renderers.extend([hline, vline])
			bokeh.plotting.output_file(filename)
			bokeh.plotting.save(p)
			
		
# startup
if __name__ == '__main__':


	# command line usage.  Pass the location of a config file as a single argument.
	parser = argparse.ArgumentParser(
		formatter_class= argparse.RawTextHelpFormatter,
		description="""Runs findNeighbour3-varmod, a service for bacterial relatedness monitoring.
									 

Example usage: 
============== 
# show command line options 
python findNeighbour3-varmod.py --help 	

# run with debug settings; only do this for unit testing.
python findNeighbour3-varmod.py 	

# run using settings in myConfigFile.json.  
python findNeighbour3-varmod.py ../config/myConfigFile.json		

""")
	parser.add_argument('path_to_config_file', type=str, action='store', nargs='?',
						help='the path to the configuration file', default='')
	
	args = parser.parse_args()
	
	# an example config file is default_test_config.json

	############################ LOAD CONFIG ######################################
	print("findNeighbour3 distance estimator .. reading configuration file.")

	if len(args.path_to_config_file)>0:
			configFile = args.path_to_config_file
	else:
			configFile = os.path.join('..','config','default_test_config.json')
			warnings.warn("No config file name supplied ; using a configuration ('default_test_config.json') suitable only for testing, not for production. ")

	# open the config file
	try:
			with open(configFile,'r') as f:
					 CONFIG=f.read()

	except FileNotFoundError:
			raise FileNotFoundError("Passed a positional parameter, which should be a CONFIG file name; tried to open a config file at {0} but it does not exist ".format(sys.argv[1]))

	if isinstance(CONFIG, str):
			CONFIG=json.loads(CONFIG)	# assume JSON string; convert.

	# check CONFIG is a dictionary	
	if not isinstance(CONFIG, dict):
			raise KeyError("CONFIG must be either a dictionary or a JSON string encoding a dictionary.  It is: {0}".format(CONFIG))
	
	# check that the keys of config are as expected.
	required_keys=set(['IP', 'REST_PORT', 'DEBUGMODE', 'LOGFILE', 'MAXN_PROP_DEFAULT'])
	missing=required_keys-set(CONFIG.keys())
	if not missing == set([]):
			raise KeyError("Required keys were not found in CONFIG. Missing are {0}".format(missing))

	# determine whether a FNPERSISTENCE_CONNSTRING environment variable is present,
	# if so, the value of this will take precedence over any values in the config file.
	# This allows 'secret' connstrings involving passwords etc to be specified without the values going into a configuraton file.
	if os.environ.get("FNPERSISTENCE_CONNSTRING") is not None:
		CONFIG["FNPERSISTENCE_CONNSTRING"] = os.environ.get("FNPERSISTENCE_CONNSTRING")
		print("Set mongodb connection string  from environment variable")
	else:
		print("Using mongodb connection string from configuration file.")

	# determine whether a FN_SENTRY_URLenvironment variable is present,
	# if so, the value of this will take precedence over any values in the config file.
	# This allows 'secret' connstrings involving passwords etc to be specified without the values going into a configuraton file.
	if os.environ.get("FN_SENTRY_URL") is not None:
		CONFIG["SENTRY_URL"] = os.environ.get("FN_SENTRY_URL")
		print("Set Sentry connection string from environment variable")
	else:
		print("Using Sentry connection string from configuration file.")
		
	########################### SET UP LOGGING #####################################  
	# create a log file if it does not exist.
	print("Starting logging")
	logdir = os.path.dirname(CONFIG['LOGFILE'])
	pathlib.Path(os.path.dirname(CONFIG['LOGFILE'])).mkdir(parents=True, exist_ok=True)

	# set up logger
	loglevel=logging.INFO
	if 'LOGLEVEL' in CONFIG.keys():
			if CONFIG['LOGLEVEL']=='WARN':
					loglevel=logging.WARN
			elif CONFIG['LOGLEVEL']=='DEBUG':
					loglevel=logging.DEBUG

	########################### prepare to launch server 
	print("Connecting to backend data store")
	try:
			PERSIST=fn3persistence(dbname = CONFIG['SERVERNAME'],
						     connString=CONFIG['FNPERSISTENCE_CONNSTRING'],
						        debug=CONFIG['DEBUGMODE'],
							server_monitoring_min_interval_msec = 0)
	except Exception as e:
			print("Error raised on creating persistence object")
			raise

	# instantiate server class
	try:
		builder = ModelBuilder(CONFIG, PERSIST)
	except Exception as e:
			print("Error raised on instantiating findNeighbour3 distance estimator object")
			raise

	print("Building model")
	model = builder.build(train_on=8000, n_components=300)
	what = ['all','ACTG']

	pcv = pcaValidator(CONFIG= CONFIG, PERSIST=PERSIST, model=model, what = what )

	pca_ceiling = pcv.validate(sample_size=200, internal=True)
	filename  = "v14_internal_sampling_output_{0}.csv"
	pcv.validation_data.to_csv(filename)

	print("Exported ",filename)
	for item in what:
		filename  = "v14_quality_info_{0}.csv".format(item)
		pcv.pcc[item].qualityScoring.to_csv(filename)
		filename  = "v14_eigenvalues_{0}.csv".format(item)
		pcv.pcc[item].pcaArray.to_csv(filename)

		print("for {1}, {0} samples exist with small SNV distances but large PCA distances".format(len(pcv.for_analysis[item].index), item))
		filename  = "v14_failed_qc_{0}.csv".format(item)
	
		pcv.for_analysis[item].to_csv(filename)
		print("Exported: ",filename)

