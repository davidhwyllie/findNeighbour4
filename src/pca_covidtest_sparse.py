#!/usr/bin/env python
""" 
A component of a findNeighbour4 server which provides relatedness information for bacterial genomes.
It does so using PCA, and supports PCA based cluster generation.

The associated classes compute a variation model for samples in a findNeighbour4 server.
Computation uses data in MongoDb, and is not memory intensive, using configuration information in a 
config file. 

If no config file is provided, it will run in  'testing' mode with the  parameters
in default_test_config.json.  This expects a mongodb database to be running on
the default port on local host. 

Two classes provide the functionality:
* VariationModel - the weights generated by a PCA (requires: nothing) 
* ModelBuilder - produces VariationModels (requires: PERSIST object for mongodb access; parameters) 

These classes are not yet properly unit tested. 
"""
 
# import libraries
import os
import io
import sys
import json
import logging
import warnings

import datetime
import glob
import time
import random
import pandas as pd
import numpy as np
import pathlib
import hashlib

import argparse
import progressbar

from Bio import Phylo

from scipy.stats import binom_test
from scipy.cluster import hierarchy
from scipy.cluster.hierarchy import dendrogram, linkage, ClusterWarning
from scipy.cluster.hierarchy import ClusterWarning

from sklearn import linear_model
from sklearn.decomposition import SparsePCA
from sklearn.cluster import DBSCAN
from sklearn.metrics.pairwise import euclidean_distances

from sentry_sdk import capture_message, capture_exception
from sentry_sdk.integrations.flask import FlaskIntegration

# ignore clusterwarnings
warnings.simplefilter("ignore", ClusterWarning)     # nonsensical cluster warnings get issued

# logging
from logging.config import dictConfig

# reference based compression storage and clustering modules
from mongoStore import fn3persistence

class MNStats():
    """ computes the number of M and N bases in a reference compressed object """
    def __init__(self, select_positions, analysed_reference_length):
        """ input:
            select_positions are the positions contributing to the pca model, as generated by ModelBuilder.
            analysed_reference_length are the number of reference bases analysed. """
        self.select_positions = select_positions
        self.analysed_reference_length = analysed_reference_length
    def examine(self, obj): 
        """ examines the reference compressed object obj,
        reporting
        * number of Ns and Ms in the sequence, subdivided by whether they are in 
          select_positions
        * "Test 2" (binomial test, as per findNeighbour3 paper) testing whether the frequency of Ns/Ms in the selected_positions exceed those elsewhere,
                   indicative of a mixture. """

        missing= {'M_in_model':0,
        'N_in_model':0,                     'model_positions':len(self.select_positions),                       'reference_positions':self.analysed_reference_length}

        for base in ['M','N']:      # compute missingness

            # record total numbers of N and M per guid
            try:
                missing["{0}_total".format(base)] = len(obj[base])
            except KeyError:
                missing["{0}_total".format(base)] = 0

            # examine all missing (N/M) sites, adding to a missingness model
            try:
                for pos in obj[base]:
                    if pos in self.select_positions:
                        try:
                            missing["{0}_in_model".format(base)] +=1                            
                        except KeyError:
                            pass

            except KeyError:
                pass        # if there are no M,N then we can ignore these

            # compute an factor which could be used to adjust an eigenvector 
            # based on the number of Ns assuming Ns are missing at random.
            try:
                missing["{0}_eigenvalue_inflation_parameter".format(base)]  = len(self.select_positions)/(len(self.select_positions)-missing["{0}_in_model".format(base)])
            except KeyError:        # no N/M
                missing["{0}_eigenvalue_inflation_parameter".format(base)]  = 1

            ## do binomial test
            not_model = self.analysed_reference_length- len(self.select_positions)
            p_expected = (missing["{0}_total".format(base)]-missing["{0}_in_model".format(base)])/not_model
            missing["{0}_expected_proportion".format(base)]=p_expected
            in_model = len(self.select_positions)
            p_observed = missing["{0}_in_model".format(base)]/len(self.select_positions)
            missing["{0}_observed_proportion".format(base)]=p_observed
            p_val = binom_test(
                missing["{0}_in_model".format(base)], 
                len(self.select_positions), 
                p_expected, 
                alternative='greater')
 
            missing["{0}_p_value".format(base)]=p_val
            
        return missing
class VariationModel():
    """ represents the output of a PCA, as performed by model builder
        You should not normally have to call this class directly to create a VariationModel - the ModelBuilder class would do this for you.
        
        - You might wish to instantiate this class directly if you are restoring a previously serialised VariationModel - see constructor""" 
        
    def __init__(self,serialised_representation = None):
        """ creates an object which contains a representation the Principal Components
           of a set of sequences, and (optionally) their clustering. 
           
           Inputs:
           serialised_representation:  either None (a new VariationModel which is empty is created) or a dictionary previously created by .serialise()


           This class can created in 3 ways:
            * an empty, if serialised representation is None, example:
            vm = VariationModel()
            - use the .add() method to add properties of the model, and .finish() when the model is built.

            * a new model, generated from data.  Such models would have been created (typically with the ModelBuilder class), e.g. as follows:
            builder = ModelBuilder(CONFIG, PERSIST)
            vm = builder.build(train_on=500, n_components=15, pca_parameters= {'n_jobs':-1})       # method returns a VariationModel object
            to_serialise = json.dumps(vm.serialise())       # to_serialise is a string which can be stored, e.g. in a database.  .serialise() returns a dictionary which is jsonable.
            
            * restore  a previously build model
            deserialise = json.loads(to_serialise)          # deserialise is a dictionary
            vm2 = VariationModel(serialised_representation= deserialise)    # create a new VariationModel           

        """
        if serialised_representation is None:
            self.model = {'built':False}
        else:
            self.model = {}
            self._deserialise(serialised_representation)    
        return
    def add(self, key, value):
        """ adds a key-value pair to the model """
        if key in self.model.keys():
            raise KeyError("Cannot replace key {0}".format(key))
        else:
            self.model[key] = value
        return
    def _coefficients_hash(self):
        """ computes a hash on the coefficients in the variant model.
        This is useful for version tracking & storing patterns of masking. """
        h = hashlib.md5()
        h.update(self.model['eigenvectors'].to_csv().encode('utf-8'))
        md5_l = h.hexdigest()
        return("{0}".format(md5_l))

    def finish(self):
        """ completes construction of the VariationModel """
        self.model['build_time'] = datetime.datetime.now().isoformat()
        self.model['coefficients_hash'] = self._coefficients_hash()
        self.model['built'] = True

    def serialise(self):
        """ serialise to model to a dictionary compatible with json serialisation """
        target_classes = dict()
        serialised = {}
        for key in vm.model:
            if not type(self.model[key]) == SparsePCA:   # we don't serialise this
                target_classes[key]= str(type(self.model[key]))

                if type(self.model[key]) in (bool,int, dict, float, list, str):
                    serialised[key]=self.model[key]
                elif type(self.model[key]) == set:
                    serialised[key]= list(self.model[key])
                elif type(self.model[key]) == np.int64:
                    serialised[key]= int(self.model[key])
                elif type(self.model[key]) == pd.core.frame.DataFrame:
                    serialised[key]= self.model[key].to_dict(orient='index')
                else:
                    warnings.warn("Not handled {0} with class {1}".format(key, type(self.model[key])))

        serialised['_target_classes'] = target_classes

        return serialised

    def _deserialise(self, deserialise):
        """ transform to native format based on dictionary produced by serialise
        Called by the constructor; do not class directly.

        deserialise: a dictionary.  Should have been converted from json string if appropriate, e.g.         
                        deserialise = json.loads(to_serialise)
        """

        if not type(deserialise) == dict:
            raise TypeError("must be passed a dictionary, not a {0}.  You may need to json.loads() the string.  ".format(type(deserialise)))
        if not '_target_classes' in deserialise.keys():
            raise KeyError("The dictionary passed doesn't contain _target_classes.  It wasn't produced by .serialise, and can't be analysed")

        for key in deserialise['_target_classes'].keys():
            if not key == '_target_classes':
                if not key in deserialise:
                    raise KeyError("Expected a key {0} in the dictionary to deserialise.  It's not there.  dictionary cannot be deserialised.  Keys are {1}".format(key, deserialise.keys()))

                # restore to native types
                target_class  = deserialise['_target_classes'][key]
                if target_class in ("<class 'dict'>",  "<class 'list'>", "<class 'str'>"):
                    # no conversion needed
                    self.model[key] = deserialise[key]
                elif target_class in ("<class 'bool'>"):
                    self.model[key] = bool(deserialise[key])
                elif target_class in ("<class 'int'>"):
                    self.model[key] = int(deserialise[key])
                elif target_class in ("<class 'float'>"):
                    self.model[key] = float(deserialise[key])
                elif target_class in ("<class 'set'>"):
                    self.model[key] = set(deserialise[key])
                elif target_class in ("<class 'pandas.core.frame.DataFrame'>"):
                    self.model[key] = pd.DataFrame.from_dict(deserialise[key], orient='index')
                else:
                    raise TypeError("Unable to deserialise, don't know how to deal with {0}/{1}".format(key,deserialise[key]))       
            
    def _getNewick(self, node, newick, parentdist, leaf_names):
        """ generates a Newick format tree file 

        Inputs:
        node: a scipy hierarchy converted to a tree, e.g. below
            # dist is a distance matrix
            dist= euclidean_distances(cluster_centres, cluster_centres)
            Z = linkage(dist, 'ward')
            t = hierarchy.to_tree(Z, False)
            nwk = getNewick(t, "", t.dist, list_of_node_names)
        newick: the existing newick string
        parentdist:  the distance matrix of the parent
        leaf_names:  leaf names of the tree

        Note: code is modified from user jfn https://stackoverflow.com/questions/28222179/save-dendrogram-to-newick-format
        this method uses recursion.
        """

        if node.is_leaf():
            return "%s:%.2f%s" % (leaf_names[node.id], parentdist - node.dist, newick)
        else:
            if len(newick) > 0:
                newick = "):%.2f%s" % (parentdist - node.dist, newick)
            else:
                newick = ");"
            newick = self._getNewick(node.get_left(), newick, node.dist, leaf_names)
            newick = self._getNewick(node.get_right(), ",%s" % (newick), node.dist, leaf_names)
            newick = "(%s" % (newick)
            return newick

    def cluster(self, eps=0.5):
        """ clusters the eigenvalues using DBSCAN. 
        This technique finds regions of high point density (i.e. where there are lots of sequences)
        in the n-dimensional space represented by the eigenvalues using the DBSCAN algorithm.  
        This produces bigger and more biologically sensible clusters than the related OPTICS algorithm. 

        Ester, M., H. P. Kriegel, J. Sander, and X. Xu, “A Density-Based Algorithm for Discovering Clusters in Large Spatial Databases with Noise”. In: 
            Proceedings of the 2nd International Conference on Knowledge Discovery and Data Mining, Portland, OR, AAAI Press, pp. 226-231. 1996
        Schubert, E., Sander, J., Ester, M., Kriegel, H. P., & Xu, X. (2017). DBSCAN revisited, revisited: why and how you should (still) use DBSCAN. 
            ACM Transactions on Database Systems (TODS), 42(3), 19.
        as implemented
        https://scikit-learn.org/stable/modules/generated/sklearn.cluster.DBSCAN.html

        Inputs:
            eps - a parameter controlling the minimum distance to a neighbouring point.  Values of 0.5 (default) seem appropriate.  Higher values give bigger clusters.
            The number of clusters is quite sensitive to the parameter and increases ~ monotonically for 0.005 < eps < 0.5 (not tested outside these ranges)

        Outputs:
            Number of clusters generated

        Sets:
            self.cluster_designations: a data frame containing cluster for each cluster
            self.cluster_centres: a data frame containging centroids of each cluster
            self.cluster_tree: a newick tree, generated by NJ method, including all the groups in the clustering

        NOTE:  this clustering method could be further parameterised to remove principal components of technical origin, if required.

        """

        # check there is a model
        if not self.model['built']:
            raise NotImplementedError("You must build a model before calling .cluster()")

        print("Clustering - running DBSCAN")
        ev = self.model['eigenvalues'].copy()        # eigenvalues.  This is the point at which PCs of technical origin could be dropped. 
        fit = DBSCAN(metric= 'l2', min_samples=2, eps=eps).fit(ev)

        ev['cluster']=fit.labels_
        clustered = ev[ev['cluster']>=0].copy()

        # relabel clusters to include sizes, as this helps understand it later
        first_column = clustered.columns.to_list()[0]
        cluster_sizes = dict(clustered.groupby('cluster')[first_column].count())
        clustered['cluster'] = ["Cluster={0}|Size={1}".format(x, cluster_sizes[x]) for x in clustered['cluster']]

        # relabel individual samples which are numbered with negative numbers (all are labelled -1) as these are singletons
        not_clustered = ev[ev['cluster']<0].copy()
        not_clustered['cluster'] = ["Sample={0}|Size={1}".format(x,1) for x in not_clustered.index]   
        self.model['cluster_designations']= pd.concat([clustered, not_clustered])

        # compute cluster centres
        print("Computing distance matrix for cluster centres")
        self.model['cluster_centres'] = self.model['cluster_designations'].groupby('cluster').mean()                  # compute centres of dimension for each cluster
        dist= euclidean_distances(self.model['cluster_centres'], self.model['cluster_centres'])     # compute distance matrix

        # use scipy's C++ implementation; BioPython is v slow in comparison.
        print("Performing hierarchical clustering using Ward's method")
        Z = linkage(dist, 'ward')
        t = hierarchy.to_tree(Z, False)
        self.model['cluster_tree'] = self._getNewick(t, "", t.dist, self.model['cluster_centres'].index.to_list())
        self.model['eigenvalue_clusters'] = ev

        return(len(self.model['cluster_centres'].index))
    
    def annotate_tree_with_cluster_output(self, newick_tree_string):
        """ Marks the result of the clustering done here on a phylogenetic tree.
        This is useful to consider whether the results of the PCA-based clustering performed here is similar to 
        that of (for example) Maximal Likelihood trees generated from the original fasta files loaded.

        For example, suppose a fasta file 'micro.fas' containing multiple genomes had been loaded.
        One could generate a ML tree using a tree building software, e.g. iqtree:

        ./iqtree2 -s micro.fas -T 6 -m GTR+I+G

        If this gave Newick format output in 'micro.fas.treefile', we could make a new version of the newick output in which
        - The cluster membership is marked in the node names
        - Whether the sample is considered mixed is marked

        Input:
            a newick string consisting of the ML tree, or any other kind of tree

        Output:
            a modified cluster string in which any nodes clustered are marked with cluster numbers, and any ? mixed samples are marked. 
        """

        # check that clustering has occurred.
        if not 'eigenvalue_clusters' in self.model.keys():
            raise KeyError("Cannot annotate tree as not clustering is stored.  Call .cluster() on VariantModel object.")

        # load tree
        with io.StringIO(newick_tree_string) as f:
            Tree=Phylo.read(f, format='newick')

        guids = set(self.model['eigenvalue_clusters'].index)
        for i,node in enumerate(Tree.get_terminals()):
            if node.name in guids:        
                node.name  = "{0} cl {1}".format(node.name, self.model['eigenvalue_clusters'].at[node.name,'cluster'])

        badguids = set(self.model['suspect_quality_seqs'].index)
        for i,node in enumerate(Tree.get_terminals()):
            if node.name in badguids:        
                node.name  = "{0} cl {1}".format(node.name, '?mixed')


        with io.StringIO(initial_value = '') as f:
            Phylo.write(Tree, f, format='newick')
            retVal = f.getvalue()

        return retVal

    
class ModelBuilder():
    """ builds a variant model from reference mapped data, and performs PCA on it """ 
        
    def __init__(self,CONFIG, PERSIST):
        """ Using values in CONFIG, estimates based on distances in the server with CONFIG['NAME'] on port CONFIG['PORT'].
        
        Note that the findNeighbour4 server itself does not have to be operational for this class to work, but the underlying database does have to be accessible.

        related to error handling:
        SENTRY_URL (optional)
        Note: if a FN_SENTRY URL environment variable is present, then the value of this will take precedence over any values in the config file.
        This allows 'secret' connstrings involving passwords etc to be specified without the values going into a configuraton file.
        PERSIST is a storage object needs to be supplied.  The fn3Persistence class in mongoStore is one suitable object.
        PERSIST=fn43persistence(connString=CONFIG['FNPERSISTENCE_CONNSTRING'])

        """
        
        # store the persistence object as part of the object
        self.PERSIST=PERSIST
        
        # check input
        if isinstance(CONFIG, str):
            self.CONFIG=json.loads(CONFIG)  # assume JSON string; convert.
        elif isinstance(CONFIG, dict):
            self.CONFIG=CONFIG
        else:
            raise TypeError("CONFIG must be a json string or dictionary, but it is a {0}".format(type(CONFIG)))
        
        # check it is a dictionary  
        if not isinstance(self.CONFIG, dict):
            raise KeyError("CONFIG must be either a dictionary or a JSON string encoding a dictionary.  It is: {0}".format(CONFIG))
        
        # check that the keys of config are as expected.
        required_keys=set(['IP','INPUTREF','EXCLUDEFILE','DEBUGMODE','SERVERNAME',
                           'FNPERSISTENCE_CONNSTRING', 'MAXN_STORAGE',
                            "SNPCEILING", 'MAXN_PROP_DEFAULT', 'REST_PORT',
                           'LOGFILE','LOGLEVEL','CLUSTERING'])
        missing=required_keys-set(self.CONFIG.keys())
        if not missing == set([]):
            raise KeyError("Required keys were not found in CONFIG. Missing are {0}".format(missing))

        # load reference
        cfg = self.PERSIST.config_read('config')
        
        # set easy to read properties from the config
        self.analysed_reference_length = len( cfg['reference']) - len(set(cfg['excludePositions']))

        # we start without any variation model
        self._reset()
        
    def _reset(self):
        """ clears existing variation model and pca result """ 
        self.vm = VariationModel()      
        self.pcs = None
        self.eigenvectors = None
        self._invalid = set()   # invalid guids for which we can't compute pcs
        self.model = {'built':False, 'built_with_guids':[]}

        self.validation_data = None

    def guids(self):
        """ returns list of guids currently in the findNeighbour4 database"""
        return sorted(self.PERSIST.refcompressedsequence_guids())   # sorting is not essential, but makes more deterministic for testing
        
    def _column_name(self,pos,base):
        """ given a base at a position, returns a position:base string suitable for use as a pandas column name """             
        return  "{0}:{1}".format(pos,base)
            
    def build(self, n_components, pca_parameters={}, min_variant_freq=None, train_on = None, deterministic = True):
        """ builds a variant model, conducts pca, and stores the output in a VariantModel object.

            input: 
                n_components: the maximum number of components to extract.  
                min_variant_freq: the minimum proportion of samples with variation at that site for the site to be included.  If none, is set to 10/train_on, i.e. each variant has to appear 10 times to be considered
                train_on: only compute PCA on a subset of train_on samples.  Set to None for all samples.
                pca_parameters: a dictionary of parameters passed to the PCA command, which in this case is SparsePCA.  
                                Example: {'n_jobs':-1} to use all processors.  The contents of the dictionary are passed as-is to the PCA command, without any checking.
            returns:
                a VariantModel object
            
        """
        
        # determine guids there in the database
        guids = self.guids()
        
        # randomise order for model training purposes if required
        if not deterministic:
            random.shuffle(guids)       # makes pipeline non-deterministic if not all samples are analysed
        
        if train_on is None:            # if we are not told how many to use then we use
            train_on = len(guids)       # all samples 
        
        # persist parameters used
        self.vm.add('train_on',train_on)
        self.vm.add('n_components', n_components)
        
        # if minimum variation is not set, only analyse variants seen at least 10 times. 
        if min_variant_freq is None:
            min_variant_freq = 10/train_on
        
        # build an array containing the amount of variation and missingness (M/N) across bases.
        vmodel = {}     # variants
        mmodel = {}     # missingness
        guids_analysed_stage1 = set()
        nLoaded = 0

        print(">>Determining variant sites, from a derivation set of up to {0} samples ".format(train_on))
        bar = progressbar.ProgressBar(max_value = train_on)
        for guid in guids:
            nLoaded+=1

            if nLoaded>=train_on:       # trained on enough samples
                break
                pass
            bar.update(nLoaded)
            obj = self.PERSIST.refcompressedsequence_read(guid) # ref compressed sequence

            if not obj['invalid'] == 1:
                guids_analysed_stage1.add(guid)
                # for definite calls, compute variation at each position
                # regards Ns as definite variation, as they may represent gaps
                for base in ['A','C','G','T']:
                    try:
                        for pos in obj[base]:
                            try:
                                vmodel[pos]=vmodel[pos]+1
                            except KeyError:
                                if not pos in vmodel.keys():
                                    vmodel[pos]={}
                                vmodel[pos]=1

                    except KeyError:
                        pass        # if there are no A,C,G,T or N then we can ignore these
                for base in ['M','N']:      # compute missingness/gaps if it's mixed or N

                    # examine all missing (N/M) sites, adding to a missingness model
                    try:
                        for pos in obj[base]:
                            try:
                                mmodel[pos]=mmodel[pos]+1
                            except KeyError:
                                if not pos in vmodel.keys():
                                    mmodel[pos]={}
                                mmodel[pos]=1

                    except KeyError:
                        pass        # if there are no M,N then we can ignore these
        bar.finish()

        # store variant model
        self.vm.add('variant_frequencies', vmodel)
        self.vm.add('min_variant_freq', min_variant_freq)
        self.vm.add('analysed_reference_length', self.analysed_reference_length)
        
        # from the totality of the the variation, select positions with > cutoff % variation
        cutoff_variant_number = nLoaded*min_variant_freq
        self.vm.add('cutoff_variant_number', cutoff_variant_number)

        select_positions = set()
        for pos in vmodel.keys():
            if vmodel[pos]>=cutoff_variant_number:

                select_positions.add(pos)
        print("Found {0} positions which vary at frequencies more than {1}.".format(len(select_positions),min_variant_freq))

        if len(select_positions) == 0:
            raise ValueError("No variation found above cutoff. normally this is because you ran the PCA operation against an empty database; this is what happens if you omit a config file parameter, when a test database is examined by default.   Cannot continue")

        self.model['variant_positions_gt_cutoff_variant_number'] = len(select_positions)

        # analyse missingness in these
        missingness = []
        for pos in select_positions:
            try:
                missingness.append(mmodel[pos])
                
            except KeyError:        # no missing data at this position
                missingness.append(0)
        mean_missingness = np.mean(missingness)
        sd_missingness = np.sqrt(mean_missingness)      # poisson assumption
        upper_cutoff = int(2*sd_missingness + mean_missingness) # normal approximation
        print("Identified max acceptable missingness as {0} positions ({1}%)".format(upper_cutoff,int(100*upper_cutoff/train_on)))
        self.vm.add('max_ok_missingness', upper_cutoff)
        self.vm.add('max_ok_missingness_pc', int(100*upper_cutoff/train_on))
    
        # remove any positions with missingness more than this
        to_remove=[]
        for pos in select_positions:
            try:
                if mmodel[pos]>upper_cutoff:
                    to_remove.append(pos)
            except KeyError:
                pass    # no missing data at this point
        for item in to_remove:
            select_positions.remove(item)
        self.select_positions = select_positions
        print("Removed {0} positions with missingness > cutoff [!! anxiety we are removing salient gaps from pca]".format(len(to_remove)))
        
        self.mns = MNStats(select_positions, self.vm.model['analysed_reference_length'] )
        print("Found {0} positions which vary at frequencies more than {1} and pass missingness cutoff.".format(len(select_positions),min_variant_freq))
        self.vm.add('variant_positions_ok_missingness', len(select_positions))

        # find any samples which have a high number of missing bases
        print(">> scanning for sequences with per-sample unexpectedly high missingness (N), or likely to be mixed (M)")
        bar = progressbar.ProgressBar(max_value = len(guids_analysed_stage1))
        
        guid2missing = {}
        for nLoaded,guid in enumerate(guids_analysed_stage1):
            bar.update(nLoaded)
            obj = self.PERSIST.refcompressedsequence_read(guid) # ref compressed sequence
            for base in ['M','N']:      # compute missingness

                # examine all missing (N/M) sites, adding to a missingness model
                try:
                    for pos in obj[base]:
                        if pos in select_positions:
                            try:
                                mmodel[pos]=mmodel[pos]+1
                            except KeyError:
                                if not pos in vmodel.keys():
                                    mmodel[pos]={}
                                mmodel[pos]=1

                except KeyError:
                    pass        # if there are no M,N then we can ignore these

            ## do binomial test and n count
            guid2missing[guid] = self.mns.examine(obj)

        bar.finish()


        # collate mixture quality information, and identify low quality (mixed, \
        # as judged by high Ns or Ms in the variant sites)
        mix_quality_info = pd.DataFrame.from_dict(guid2missing, orient='index')
        self.vm.add('mix_quality_info',mix_quality_info)

        # identify any mixed samples.  we don't build the model from these.
        # mixed are defined as having significantly more N or M in the variant
        # positions than in other bases.

        ### CAUTION - not sure whether this is applicable to SARS-COV-2
        mix_quality_cutoff = 0.05 / len(mix_quality_info.index)     # 0.05 divided by the number of mixed samples?  Bonferroni adj; maybe better to use FDR      
        suspect_quality = mix_quality_info.query('M_p_value < {0} or N_p_value < {0}'.format(mix_quality_cutoff))
        self.vm.add('suspect_quality_seqs', suspect_quality)
        print(">> Identified {0} sequences based on their having unexpectedly higher Ns in variant vs. non-variant bases; excluded from model as may be mixed.".format(len(set(suspect_quality.index.to_list()))))

        guids_analysed_stage2= guids_analysed_stage1 - set(suspect_quality.index.to_list())

        # build a variation matrix for variant sites
        vmodel = {}
        nLoaded = 0
        print(">>Gathering variation for matrix construction from {0} unmixed samples ".format(len(guids_analysed_stage2)))

        bar = progressbar.ProgressBar(max_value = train_on)
        
        self.model['built_with_guids'] = []
        for guid in guids_analysed_stage2:
            nLoaded+=1
            if nLoaded>=train_on:       # if we're told only to use a proportion, and we've analysed enough,
                break
                pass
            bar.update(nLoaded)

            obj = self.PERSIST.refcompressedsequence_read(guid) # ref compressed sequence
            # for definite calls, compute variation at each position
            
            # for invalid samples, compute nothing
            if obj['invalid'] == 1:
                self._invalid.add(guid)
            else:
                # compute a variation model - a list of bases and variants where variation occurs
                variants={}     # variation for this guid

                # for definite calls, compute variation at each position
                # positions of variation where a call was made
                for base in set(['A','C','G','T']).intersection(obj.keys()):
                    target_positions = select_positions.intersection(obj[base])
                    called_positions = dict((self._column_name(pos,base),1) for pos in target_positions) 
                    
                    variants = {**variants, **called_positions}                   
                    
                vmodel[guid]=variants
        bar.finish()

        print(">>Building variant matrix")  
        vmodel = pd.DataFrame.from_dict(vmodel, orient='index')
        vmodel.fillna(value=0, inplace=True)    # if not completed, then it's reference
                                                # unless it's null, which we are ignoring at present
        #vmodel = vmodel.drop_duplicates()      # deduplicate by row - not an appropriate thing to do 
        #self.vm.add('null_elements_in_vmodel', vmodel.isna().sum().sum())
        self.vm.add('vmodel', vmodel)
        
        print(">>Performing sparse pca extracting {0} components".format(n_components))
        self.vm.add('n_pca_components', n_components)
            
        pca = SparsePCA(n_components=n_components, **pca_parameters)     # if too large, can use IncrementalPCA
        pca.fit(vmodel)
        contribs = []
        nz_columns = {}

        # summarise the positions and variants responsible for each pc
        pc2contributing_pos = {}
        contributing_basepos = set()
        contributing_pos = set()
        for i,row in enumerate(pca.components_,0):
            pc2contributing_pos[i] = set()
            for j,cell in enumerate(row,0):
                if not cell == 0:
                    pos = int(vmodel.columns[j].split(':')[0])
                    allele = vmodel.columns[j].split(':')[1]
                    pc2contributing_pos[i].add(pos)
                    contributing_basepos.add(vmodel.columns[j])
                    contributing_pos.add(pos)
                    contribs.append({'pc':i, 'pos':pos, 'allele': allele,'col':vmodel.columns[j], 'weight':cell})
            pc2contributing_pos[i] = sorted(list(pc2contributing_pos[i]))       # can be json serialised, unlike set
                
        # eigenvectors
        self.eigenvectors = pd.DataFrame.from_records(contribs)
        if len(self.eigenvectors.index)==0:
            raise KeyError("PCA problem.  No eigenvectors found.  Contributions found are as follows: {0}.  This usually means there is insufficient data to build PCs.  Try increasing the sample number".format(contribs))
        
        # compute eigenvalues for the data on which the fit was performed.
        print(">>Computing eigenvalues for {0} unmixed samples ".format(len(guids_analysed_stage2)))
        eigenvalues_dict = {}
        for guid, evs in zip(vmodel.index, pca.transform(vmodel)):
            eigenvalues_dict[guid] = evs
        eigenvalues = pd.DataFrame.from_dict(eigenvalues_dict, orient= 'index')
        eigenvalues.columns = range(n_components)

        self.vm.add('pca', pca)
        self.vm.add('eigenvalues', eigenvalues) 
        self.vm.add('eigenvectors', self.eigenvectors)
        self.vm.add('n_contributing_positions', len(contributing_pos))
        self.vm.add('pc2_contributing_positions', pc2contributing_pos)
        self.vm.add('n_contributing_variants', len(contributing_basepos))
        self.vm.add('contributing_basepos', contributing_basepos)
        self.vm.add('contributing_pos', contributing_pos)
        self.vm.add('built_with_guids', vmodel.index.tolist())
        self.vm.add('pos_per_pc', [len(x) for x in self.vm.model['pc2_contributing_positions'].values()])
       

        print("PCA completed, identified {0} contributing base/positions".format(len(contributing_basepos)))
        self.vm.finish()
        
        return self.vm
            
        
# startup
if __name__ == '__main__':


    # command line usage.  Pass the location of a config file as a single argument.
    parser = argparse.ArgumentParser(
        formatter_class= argparse.RawTextHelpFormatter,
        description="""Runs findNeighbour3-varmod, a service for bacterial relatedness monitoring.
                                     

Example usage: 
============== 
# show command line options 
python findNeighbour3-varmod.py --help  

# run with debug settings; only do this for unit testing.
python findNeighbour3-varmod.py     

# run using settings in myConfigFile.json.  
python findNeighbour3-varmod.py ../config/myConfigFile.json     

""")
    parser.add_argument('path_to_config_file', type=str, action='store', nargs='?',
                        help='the path to the configuration file', default='')
    
    args = parser.parse_args()
    
    # an example config file is default_test_config.json

    ############################ LOAD CONFIG ######################################
    print("findNeighbour3 distance estimator .. reading configuration file.")

    if len(args.path_to_config_file)>0:
            configFile = args.path_to_config_file
    else:
            configFile = os.path.join('..','config','default_test_config.json')
            warnings.warn("No config file name supplied ; using a configuration ('default_test_config.json') suitable only for testing, not for production. ")

    # open the config file
    try:
            with open(configFile,'r') as f:
                     CONFIG=f.read()

    except FileNotFoundError:
            raise FileNotFoundError("Passed a positional parameter, which should be a CONFIG file name; tried to open a config file at {0} but it does not exist ".format(sys.argv[1]))

    if isinstance(CONFIG, str):
            CONFIG=json.loads(CONFIG)   # assume JSON string; convert.

    # check CONFIG is a dictionary  
    if not isinstance(CONFIG, dict):
            raise KeyError("CONFIG must be either a dictionary or a JSON string encoding a dictionary.  It is: {0}".format(CONFIG))
    
    # check that the keys of config are as expected.
    required_keys=set(['IP', 'REST_PORT', 'DEBUGMODE', 'LOGFILE', 'MAXN_PROP_DEFAULT'])
    missing=required_keys-set(CONFIG.keys())
    if not missing == set([]):
            raise KeyError("Required keys were not found in CONFIG. Missing are {0}".format(missing))

    # determine whether a FNPERSISTENCE_CONNSTRING environment variable is present,
    # if so, the value of this will take precedence over any values in the config file.
    # This allows 'secret' connstrings involving passwords etc to be specified without the values going into a configuraton file.
    if os.environ.get("FNPERSISTENCE_CONNSTRING") is not None:
        CONFIG["FNPERSISTENCE_CONNSTRING"] = os.environ.get("FNPERSISTENCE_CONNSTRING")
        print("Set mongodb connection string  from environment variable")
    else:
        print("Using mongodb connection string from configuration file.")

    # determine whether a FN_SENTRY_URLenvironment variable is present,
    # if so, the value of this will take precedence over any values in the config file.
    # This allows 'secret' connstrings involving passwords etc to be specified without the values going into a configuraton file.
    if os.environ.get("FN_SENTRY_URL") is not None:
        CONFIG["SENTRY_URL"] = os.environ.get("FN_SENTRY_URL")
        print("Set Sentry connection string from environment variable")
    else:
        print("Using Sentry connection string from configuration file.")
        
    ########################### SET UP LOGGING #####################################  
    # create a log file if it does not exist.
    print("Starting logging")
    logdir = os.path.dirname(CONFIG['LOGFILE'])
    pathlib.Path(os.path.dirname(CONFIG['LOGFILE'])).mkdir(parents=True, exist_ok=True)

    # set up logger
    loglevel=logging.INFO
    if 'LOGLEVEL' in CONFIG.keys():
            if CONFIG['LOGLEVEL']=='WARN':
                    loglevel=logging.WARN
            elif CONFIG['LOGLEVEL']=='DEBUG':
                    loglevel=logging.DEBUG

    ########################### prepare to launch server 
    print("Connecting to backend data store")
    try:
            PERSIST=fn3persistence(dbname = CONFIG['SERVERNAME'],
                             connString=CONFIG['FNPERSISTENCE_CONNSTRING'],
                                debug=CONFIG['DEBUGMODE'],
                            server_monitoring_min_interval_msec = 0)
    except Exception as e:
            print("Error raised on creating persistence object")
            raise

    # instantiate server class
    try:
        builder = ModelBuilder(CONFIG, PERSIST)
    except Exception as e:
            print("Error raised on instantiating findNeighbour3 distance estimator object")
            raise

    print("Building model")
    vm = builder.build(train_on=None, n_components=150, pca_parameters= {'n_jobs':-1})       # run all   


    # cluster
    print("Clustering Eigenvalues")
    ncl = vm.cluster()
    print("Identified {0} clusters".format(ncl))
    # serialise
    to_serialise = json.dumps(vm.serialise())

    # deserialise
    deserialise = json.loads(to_serialise) 
    vm2 = VariationModel(serialised_representation= deserialise)           
    
    print("Clustering")
    ev = vm.model['eigenvalues']        # eigenvalues

    filename = '/srv/data/mixfiles/covid/milk_micro.fas.treefile'       # ML tree of a 2500 sample subset
    with (open(filename,'r')) as f:
        ml_tree_string = f.read()

    annotated_ml_tree_string = vm.annotate_tree_with_cluster_output(ml_tree_string)

    filename = 'output.treefile'
    with (open(filename,'w')) as f:
        f.write(annotated_ml_tree_string) 

    print('Finished - output in output.treefile')
    exit()
 
    